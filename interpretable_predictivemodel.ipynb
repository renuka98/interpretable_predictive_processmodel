{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPM2020_Attention",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhvyI7kz3QfA",
        "colab_type": "text"
      },
      "source": [
        "# Building accurate and interpretable models for predictive process analytics\n",
        "This notebook has all the experiments conducted in the paper. The work re-uses many elements of the experimental setting developed by the authors of \"Learning Accurate LSTM Models of Business Processes\" https://github.com/AdaptiveBProcess/GenerativeLSTM\n",
        "\n",
        "We are grateful to the authors for sharing the code thus supporting reproducability of the results, and extensions on using deep learning techniques for predictive process monitoring.\n",
        "\n",
        "We reuse the following to compare the performance of the models:\n",
        "1. Preprocessing of the logs to generate model inputs\n",
        "2. Role Identification as a feature\n",
        "3. Suffix Generation and distance computation\n",
        "\n",
        "The main contribution of our work is:\n",
        "1. Model Prefix attention and interpretation\n",
        "2. Model attribute attention and interpretation\n",
        "3. Ablation study of features\n",
        "4. Remaining time prediction using attentional neural networks\n",
        "\n",
        "The attention implementation is an improved version of Reverse Time Attention Mechanism as discussed in the paper: \n",
        "\n",
        "Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, Jimeng Sun, 2016, RETAIN: An interpretable predictive model for healthcare using reverse time attention mechanism, In Proc. of Neural Information Processing Systems (NIPS) 2016, pp.3504-3512. \n",
        "(implementation at https://github.com/Optum/retain-keras)\n",
        "\n",
        "To evaluate and run the results\n",
        "1. Step 1 - Execute model training - you can configure l_size: # of lstm cells and n_size: # of sequences or prefix length, model types \n",
        "    -- prefix_attn : Attention for prefix\n",
        "    -- full_attn : Attention for prefix and variables\n",
        "    -- default: Full attention precting next activity and time\n",
        "2. Step 2 - Evaluate model for next activity\n",
        "3. Step 3 - Evaluate the model for suffix and remaining time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBCCeX-eYvKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils.data_utils import Sequence\n",
        "from keras.regularizers import l2\n",
        "from keras.constraints import non_neg, Constraint\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "\n",
        "\n",
        "from keras.layers import Input, Concatenate, Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.optimizers import Nadam, Adam, SGD, Adagrad\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muSHnCvgkazO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install jellyfish\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6cYvZsnnTrO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Utility functions (previous work)\n",
        "Utilities used for pre-processing of event logs - Used in previous work - author \n",
        "\n",
        "support.py\n",
        "\n",
        "\n",
        " Author - Manuel Camargo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDLCG33rk35H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from sys import stdout\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import uuid\n",
        "import json\n",
        "import platform as pl\n",
        "\n",
        "\n",
        "def folder_id():\n",
        "    return datetime.datetime.today().strftime('%Y%m%d_%H%M%S%f')\n",
        "#generate unique bimp element ids\n",
        "def gen_id():\n",
        "    return \"qbp_\" + str(uuid.uuid4())\n",
        "\n",
        "def print_performed_task(text):\n",
        "    stdout.write(\"\\r%s\" % text + \"...      \")\n",
        "    stdout.flush()\n",
        "\n",
        "def print_done_task():\n",
        "    stdout.write(\"[DONE]\")\n",
        "    stdout.flush()\n",
        "    stdout.write(\"\\n\")\n",
        "\n",
        "def file_size(path_file):\n",
        "    size = 0\n",
        "    file_exist = os.path.exists(path_file)\n",
        "    if file_exist:\n",
        "        size = len(open(path_file).readlines())\n",
        "    return size\n",
        "\n",
        "#printing formated float\n",
        "def ffloat(num, dec):\n",
        "    return float(\"{0:.2f}\".format(np.round(num,decimals=dec)))\n",
        "\n",
        "#transform a string into date object\n",
        "#def get_time_obj(date, timeformat):\n",
        "#    date_modified = datetime.datetime.strptime(date,timeformat)\n",
        "#    return date_modified\n",
        "\n",
        "\n",
        "#print debuging csv file\n",
        "def create_csv_file(index, output_file, mode='w'):\n",
        "    with open(output_file, mode) as f:\n",
        "        for element in index:\n",
        "            w = csv.DictWriter(f, element.keys())\n",
        "            w.writerow(element)\n",
        "        f.close()\n",
        "\n",
        "def create_csv_file_header(index, output_file, mode='w'):\n",
        "    with open(output_file, mode, newline='') as f:\n",
        "        fieldnames = index[0].keys()\n",
        "        w = csv.DictWriter(f, fieldnames)\n",
        "        w.writeheader()\n",
        "        for element in index:\n",
        "            w.writerow(element)\n",
        "        f.close()\n",
        "\n",
        "def create_json(dictionary, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "         f.write(json.dumps(dictionary))\n",
        "         \n",
        "# rounding lists values preserving the sum values\n",
        "def round_preserve(l,expected_sum):\n",
        "    actual_sum = sum(l)\n",
        "    difference = round(expected_sum - actual_sum,2)\n",
        "    if difference > 0.00:\n",
        "        idx= l.index(min(l))\n",
        "    else:\n",
        "        idx= l.index(max(l))\n",
        "    l[idx] +=difference\n",
        "    return l\n",
        "\n",
        "def copy(source, destiny):\n",
        "    if pl.system().lower() == 'windows':\n",
        "        os.system('copy \"' + source + '\" \"' + destiny + '\"')\n",
        "    else:\n",
        "        os.system('cp \"' + source + '\" \"' + destiny + '\"')\n",
        "\n",
        "## added code to save figure\n",
        "def plot_history( plt, figure_name, path, save_fig=True ):\n",
        "  \n",
        "  fig_name = figure_name + \".png\"\n",
        "  full_path = path + fig_name\n",
        "  if save_fig:\n",
        "    plt.savefig(full_path, dpi=300)\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87jfyB3-BYey",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "nn_support.py\n",
        "author: Manuel Camargo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8mHSARVBbBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Nov 21 16:53:16 2018\n",
        "This module contains support functions specifically created to manipulate \n",
        "Event logs in pandas dataframe format\n",
        "@author: Manuel Camargo\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =============================================================================\n",
        "# Split an event log dataframe to peform split-validation \n",
        "# =============================================================================\n",
        "def split_train_test(df, percentage):\n",
        "    cases = df.caseid.unique()\n",
        "    num_test_cases = int(np.round(len(cases)*percentage))\n",
        "    test_cases = cases[:num_test_cases]\n",
        "    train_cases = cases[num_test_cases:]\n",
        "    df_train, df_test = pd.DataFrame(), pd.DataFrame()\n",
        "    for case in train_cases:\n",
        "        df_train = df_train.append(df[df.caseid==case]) \n",
        "    df_train = df_train.sort_values('start_timestamp', ascending=True).reset_index(drop=True)\n",
        " \n",
        "    for case in test_cases:\n",
        "        df_test = df_test.append(df[df.caseid==case]) \n",
        "    df_test = df_test.sort_values('start_timestamp', ascending=True).reset_index(drop=True)\n",
        "    \n",
        "    return df_train, df_test \n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Reduce the loops of a trace joining contiguous activities \n",
        "# exectuted by the same resource   \n",
        "# =============================================================================\n",
        "def reduce_loops(df):\n",
        "    df_group = df.groupby('caseid')\n",
        "    reduced = list()\n",
        "    for name, group in df_group:\n",
        "        temp_trace = list()\n",
        "        group = group.sort_values('start_timestamp', ascending=True).reset_index(drop=True)\n",
        "        temp_trace.append(dict(caseid=name, \n",
        "                          task=group.iloc[0].task, \n",
        "                          user=group.iloc[0].user, \n",
        "                          start_timestamp=group.iloc[0].start_timestamp, \n",
        "                          end_timestamp=group.iloc[0].end_timestamp, \n",
        "                          role=group.iloc[0].role))\n",
        "        for i in range(1, len(group)):\n",
        "            if group.iloc[i].task == temp_trace[-1]['task'] and group.iloc[i].user == temp_trace[-1]['user']:\n",
        "                temp_trace[-1]['end_timestamp'] = group.iloc[i].end_timestamp\n",
        "            else:\n",
        "                temp_trace.append(dict(caseid=name, \n",
        "                                  task=group.iloc[i].task, \n",
        "                                  user=group.iloc[i].user, \n",
        "                                  start_timestamp=group.iloc[i].start_timestamp, \n",
        "                                  end_timestamp=group.iloc[i].end_timestamp, \n",
        "                                  role=group.iloc[i].role))\n",
        "        reduced.extend(temp_trace)\n",
        "    return pd.DataFrame.from_records(reduced) \n",
        "\n",
        "# =============================================================================\n",
        "# Calculate duration and time between activities\n",
        "# =============================================================================\n",
        "def calculate_times(df):\n",
        "   # Duration\n",
        "   get_seconds = lambda x: x.seconds\n",
        "   df['dur'] = (df.end_timestamp-df.start_timestamp).apply(get_seconds)\n",
        "   # Time between activities per trace\n",
        "   df['tbtw'] = 0\n",
        "   # Multitasking time\n",
        "   cases = df.caseid.unique()\n",
        "   for case in cases:\n",
        "       trace = df[df.caseid==case].sort_values('start_timestamp', ascending=True)\n",
        "       for i in range(1,len(trace)):\n",
        "           row_num = trace.iloc[i].name\n",
        "           tbtw = (trace.iloc[i].start_timestamp - trace.iloc[i - 1].end_timestamp).seconds\n",
        "           df.iloc[row_num,df.columns.get_loc('tbtw')] = tbtw\n",
        "   return df, cases\n",
        "\n",
        "# =============================================================================\n",
        "# Standardization\n",
        "# =============================================================================\n",
        "\n",
        "def max_min_std(df, serie):\n",
        "    max_value, min_value = np.max(df[serie]), np.min(df[serie])\n",
        "    std = lambda x: (x[serie] - min_value) / (max_value - min_value)\n",
        "    df[serie+'_norm']=df.apply(std,axis=1)\n",
        "    return df, max_value, min_value\n",
        "\n",
        "\n",
        "def max_std(df, serie):\n",
        "    max_value, min_value = np.max(df[serie]), np.min(df[serie])\n",
        "    std = lambda x: x[serie] / max_value\n",
        "    df[serie+'_norm']=df.apply(std,axis=1)\n",
        "    return df, max_value, min_value\n",
        "\n",
        "def max_min_de_std(val, max_value, min_value):\n",
        "    true_value = (val * (max_value - min_value)) + min_value\n",
        "    return true_value\n",
        "\n",
        "def max_de_std(val, max_value, min_value):\n",
        "    true_value = val * max_value \n",
        "    return true_value\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TNbAuTBBitg",
        "colab_type": "text"
      },
      "source": [
        "#### Role discovery (previous work)\n",
        "We reuse the code to generate resource roles for the resources. \n",
        "\n",
        "author: Manuel Camargo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D33bglXEBkvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import scipy\n",
        "from scipy.stats import pearsonr\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "#from support_modules import support as sup\n",
        "from operator import itemgetter\n",
        "import random\n",
        "\n",
        "\n",
        "# == support\n",
        "def random_color(size):\n",
        "    number_of_colors = size\n",
        "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]\n",
        "    return color\n",
        "\n",
        "def find_index(dictionary, value):\n",
        "    finish = False\n",
        "    i = 0\n",
        "    resp = -1\n",
        "    while i<len(dictionary) and not finish:\n",
        "        if dictionary[i]['data']==value:\n",
        "            resp = dictionary[i]['index']\n",
        "            finish = True\n",
        "        i+=1\n",
        "    return resp\n",
        "\n",
        "def det_freq_matrix(unique, dictionary):\n",
        "    freq_matrix = list()\n",
        "    for u in unique:\n",
        "        freq = 0\n",
        "        for d in dictionary:\n",
        "            if u == d:\n",
        "                freq += 1\n",
        "        freq_matrix.append(dict(task=u[0],user=u[1],freq=freq))\n",
        "    return freq_matrix\n",
        "\n",
        "def build_profile(users,freq_matrix,prof_size):\n",
        "    profiles=list()\n",
        "    for user in users:\n",
        "        exec_tasks = list(filter(lambda x: x['user']==user['index'],freq_matrix))\n",
        "        profile = [0,] * prof_size\n",
        "        for exec_task in exec_tasks:\n",
        "            profile[exec_task['task']]=exec_task['freq']\n",
        "        profiles.append(dict(user=user['index'],profile=profile))\n",
        "    return profiles\n",
        "\n",
        "def det_correlation_matrix(profiles):\n",
        "    correlation_matrix = list()\n",
        "    for profile_x in profiles:\n",
        "        for profile_y in profiles:\n",
        "            x = scipy.array(profile_x['profile'])\n",
        "            y = scipy.array(profile_y['profile'])\n",
        "            r_row, p_value = pearsonr(x, y)\n",
        "            correlation_matrix.append(dict(x=profile_x['user'],y=profile_y['user'],distance=r_row))\n",
        "    return correlation_matrix\n",
        "\n",
        "# =============================================================================\n",
        "# def graph_network(g):\n",
        "#     pos = nx.spring_layout(g, k=0.5,scale=10)\n",
        "#     nx.draw_networkx(g,pos,node_size=200,with_labels=True,font_size=11, font_color='#A0CBE2')\n",
        "#     edge_labels=dict([((u,v,),round(d['weight'],2)) for u,v,d in g.edges(data=True)])\n",
        "#     nx.draw_networkx_edge_labels(g,pos,edge_labels=edge_labels)\n",
        "#     plt.draw()\n",
        "#     plt.show()\n",
        "# \n",
        "# =============================================================================\n",
        "def graph_network(g, sub_graphs):\n",
        "    #IDEA se debe calcular el centroide de los clusters....pos es un diccionario de posiciones y el centroide es el promedio de los puntos x y y\n",
        "    #despues se debe determinar el punto mas lejano del centroide y ese sera el radio y con esos datos pintar un circulo con patches\n",
        "    pos = nx.spring_layout(g, k=0.5,scale=10)\n",
        "    color = random_color(len(sub_graphs))\n",
        "    for i in range(0,len(sub_graphs)):\n",
        "        subgraph = sub_graphs[i]\n",
        "        nx.draw_networkx_nodes(g,pos, nodelist=list(subgraph), node_color=color[i], node_size=200, alpha=0.8)\n",
        "        nx.draw_networkx_edges(g,pos,width=1.0,alpha=0.5)\n",
        "        nx.draw_networkx_edges(g,pos, edgelist=subgraph.edges, width=8,alpha=0.5,edge_color=color[i])\n",
        "    plt.draw()\n",
        "    plt.show() # display\n",
        "\n",
        "def connected_component_subgraphs(G):\n",
        "    for c in nx.connected_components(G):\n",
        "        yield G.subgraph(c)\n",
        "\n",
        "def role_definition(sub_graphs,users):\n",
        "    records= list()\n",
        "    for i in range(0,len(sub_graphs)):\n",
        "        users_names = list()\n",
        "        for user in sub_graphs[i]:\n",
        "            users_names.append(list(filter(lambda x: x['index']==user,users))[0]['data'])\n",
        "        records.append(dict(role='Role '+ str(i + 1),quantity =len(sub_graphs[i]),members=users_names))\n",
        "    #Sort roles by number of resources\n",
        "    records = sorted(records, key=itemgetter('quantity'), reverse=True)\n",
        "    for i in range(0,len(records)):\n",
        "        records[i]['role']='Role '+ str(i + 1)\n",
        "    resource_table = list()\n",
        "    for record in records:\n",
        "        for member in record['members']:\n",
        "            resource_table.append(dict(role=record['role'], resource=member))\n",
        "    return records, resource_table\n",
        "# --kernel--\n",
        "\n",
        "def role_discovery(data, drawing, sim_percentage):\n",
        "    tasks = list(set(list(map(lambda x: x[0], data))))\n",
        "    try:\n",
        "        tasks.remove('Start')\n",
        "    except Exception:\n",
        "    \tpass\n",
        "    tasks = [dict(index=i,data=tasks[i]) for i in range(0,len(tasks))]\n",
        "    users = list(set(list(map(lambda x: x[1], data))))\n",
        "    try:\n",
        "        users.remove('Start')\n",
        "    except Exception:\n",
        "    \tpass\n",
        "    users = [dict(index=i,data=users[i]) for i in range(0,len(users))]\n",
        "    data_transform = list(map(lambda x: [find_index(tasks, x[0]),find_index(users, x[1])], data ))\n",
        "    unique = list(set(tuple(i) for i in data_transform))\n",
        "    unique = [list(i) for i in unique]\n",
        "    # [print(uni) for uni in users]\n",
        "    # building of a task-size profile of task execution per resource\n",
        "    profiles = build_profile(users,det_freq_matrix(unique,data_transform),len(tasks))\n",
        "    print_performed_task('Analysing resource pool ')\n",
        "#    sup.print_progress(((20 / 100)* 100),'Analysing resource pool ')\n",
        "    # building of a correlation matrix between resouces profiles\n",
        "    correlation_matrix = det_correlation_matrix(profiles)\n",
        "#    sup.print_progress(((40 / 100)* 100),'Analysing resource pool ')\n",
        "    # creation of a relation network between resouces\n",
        "    g = nx.Graph()\n",
        "    for user in users:\n",
        "        g.add_node(user['index'])\n",
        "    for relation in correlation_matrix:\n",
        "        # creation of edges between nodes excluding the same element correlation\n",
        "        # and those below the 0.7 threshold of similarity\n",
        "        if relation['distance'] > sim_percentage and relation['x']!=relation['y'] :\n",
        "            g.add_edge(relation['x'],relation['y'],weight=relation['distance'])\n",
        "#    sup.print_progress(((60 / 100)* 100),'Analysing resource pool ')\n",
        "    # extraction of fully conected subgraphs as roles\n",
        "    sub_graphs = list(connected_component_subgraphs(g))\n",
        "#    sup.print_progress(((80 / 100)* 100),'Analysing resource pool ')\n",
        "    # role definition from graph\n",
        "    roles = role_definition(sub_graphs,users)\n",
        "    # plot creation (optional)\n",
        "    if drawing == True:\n",
        "        graph_network(g, sub_graphs)\n",
        "#    sup.print_progress(((100 / 100)* 100),'Analysing resource pool ')\n",
        "    print_done_task()\n",
        "    return roles\n",
        "\n",
        "def read_roles_from_columns(raw_data, filtered_data, separator):\n",
        "\trecords = list()\n",
        "\trole_list= list()\n",
        "\tpool_list= list()\n",
        "\traw_splited= list()\n",
        "\tfor row in raw_data:\n",
        "\t\ttemp = row.split(separator)\n",
        "\t\tif temp[0] != 'End':\n",
        "\t\t\traw_splited.append(dict(role=temp[1],resource=temp[0]))\n",
        "\tfor row in filtered_data:\n",
        "\t\ttemp = row.split(separator)\n",
        "\t\tif temp[0] != 'End':\n",
        "\t\t\tpool_list.append(dict(role=temp[1],resource=temp[0]))\n",
        "\t\t\trole_list.append(temp[1])\n",
        "\trole_list = list(set(role_list))\n",
        "\tfor role in role_list:\n",
        "\t\tmembers = list(filter(lambda person: person['role'] == role, pool_list))\n",
        "\t\tmembers = list(map(lambda x: x['resource'],members))\n",
        "\t\tquantity = len(members)\n",
        "\t\t#freq = len(list(filter(lambda person: person['role'] == role, raw_splited)))\n",
        "\t\trecords.append(dict(role=role,quantity =quantity,members=members))\n",
        "\treturn records\n",
        "\n",
        "def read_resource_pool(log, separator=None, drawing=False, sim_percentage=0.7):\n",
        "    if separator == None:\n",
        "        filtered_list = list()\n",
        "        for row in log.data:\n",
        "            if row['task'] != 'End' and row['user'] != 'AUTO':\n",
        "                filtered_list.append([row['task'],row['user']])\n",
        "        return role_discovery(filtered_list, drawing, sim_percentage)\n",
        "    else:\n",
        "        raw_list = list()\n",
        "        filtered_list = list()\n",
        "        for row in log.data:\n",
        "            raw_list.append(row['user'])\n",
        "        filtered_list = list(set(raw_list))\n",
        "        return read_roles_from_columns(raw_list, filtered_list, separator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxFqdYt3n3TV",
        "colab_type": "text"
      },
      "source": [
        "#### Log reader (previous work)\n",
        "Log reader to process the logfile and generate the list of events. The log reader considers activity, resource, timestamp and lifecycle of each event.\n",
        "\n",
        "author: Manuel Carmago"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRB2qr1Xb6xE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import csv\n",
        "import datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "import gzip\n",
        "import zipfile as zf\n",
        "import os\n",
        "from operator import itemgetter\n",
        "\n",
        "class LogReader(object):\n",
        "    \"\"\"\n",
        "\tThis class reads and parse the elements of a given process log in format .xes or .csv\n",
        "\t\"\"\"\n",
        "\n",
        "    def __init__(self, input, start_timeformat, end_timeformat, log_columns_numbers=[], ns_include=True, one_timestamp=False):\n",
        "        \"\"\"constructor\"\"\"\n",
        "        self.input = input\n",
        "        self.data, self.raw_data = self.load_data_from_file(log_columns_numbers, start_timeformat, end_timeformat, ns_include, one_timestamp)\n",
        "\n",
        "\n",
        "    # Support Method\n",
        "    def define_ftype(self):\n",
        "        filename, file_extension = os.path.splitext(self.input)\n",
        "        if file_extension == '.xes' or file_extension == '.csv' or file_extension == '.mxml' :\n",
        "             filename = filename + file_extension\n",
        "             file_extension = file_extension\n",
        "        elif file_extension == '.gz':\n",
        "            outFileName = filename\n",
        "            filename, file_extension = self.decompress_file_gzip(self.input, outFileName)\n",
        "        elif file_extension=='.zip':\n",
        "            filename,file_extension = self.decompress_file_zip(self.input, filename)\n",
        "        elif not (file_extension == '.xes' or file_extension == '.csv' or file_extension == '.mxml'):\n",
        "            raise IOError('file type not supported')\n",
        "        return filename,file_extension\n",
        "\n",
        "    # Decompress .gz files\n",
        "    def decompress_file_gzip(self,filename, outFileName):\n",
        "        inFile = gzip.open(filename, 'rb')\n",
        "        outFile = open(outFileName,'wb')\n",
        "        outFile.write(inFile.read())\n",
        "        inFile.close()\n",
        "        outFile.close()\n",
        "        _, fileExtension = os.path.splitext(outFileName)\n",
        "        return outFileName,fileExtension\n",
        "\n",
        "    # Decompress .zip files\n",
        "    def decompress_file_zip(self, filename, outfilename):\n",
        "        with zf.ZipFile(filename,\"r\") as zip_ref:\n",
        "            zip_ref.extractall(\"../inputs/\")\n",
        "        _, fileExtension = os.path.splitext(outfilename)\n",
        "        return outfilename, fileExtension\n",
        "\n",
        "    # Reading methods\n",
        "    def load_data_from_file(self, log_columns_numbers, start_timeformat, end_timeformat, ns_include, one_timestamp):\n",
        "        \"\"\"reads all the data from the log depending the extension of the file\"\"\"\n",
        "        temp_data = list()\n",
        "        filename, file_extension = self.define_ftype()\n",
        "        if file_extension == '.xes':\n",
        "            temp_data, raw_data = self.get_xes_events_data(filename,start_timeformat, end_timeformat, ns_include, one_timestamp)\n",
        "        elif file_extension == '.csv':\n",
        "            temp_data, raw_data = self.get_csv_events_data(log_columns_numbers, start_timeformat, end_timeformat)\n",
        "        elif file_extension == '.mxml':\n",
        "            temp_data, raw_data = self.get_mxml_events_data(filename,start_timeformat, end_timeformat)\n",
        "        return temp_data, raw_data\n",
        "\n",
        "    def get_xes_events_data(self, filename,start_timeformat, end_timeformat, ns_include, one_timestamp):\n",
        "        \"\"\"reads and parse all the events information from a xes file\"\"\"\n",
        "        temp_data = list()\n",
        "        tree = ET.parse(filename)\n",
        "        root = tree.getroot()\n",
        "        if ns_include:\n",
        "            #TODO revisar como poder cargar el mane space de forma automatica del root\n",
        "            ns = {'xes': root.tag.split('}')[0].strip('{')}\n",
        "            tags = dict(trace='xes:trace',string='xes:string',event='xes:event',date='xes:date')\n",
        "        else:\n",
        "            ns = {'xes':''}\n",
        "            tags = dict(trace='trace',string='string',event='event',date='date')\n",
        "        traces = root.findall(tags['trace'], ns)\n",
        "        i = 0\n",
        "        print_performed_task('Reading log traces ')\n",
        "        for trace in traces:\n",
        "#            sup.print_progress(((i / (len(traces) - 1)) * 100), 'Reading log traces ')\n",
        "            caseid = ''\n",
        "            for string in trace.findall(tags['string'], ns):\n",
        "                if string.attrib['key'] == 'concept:name':\n",
        "                    caseid = string.attrib['value']\n",
        "            for event in trace.findall(tags['event'], ns):\n",
        "                task = ''\n",
        "                user = ''\n",
        "                event_type = ''\n",
        "                complete_timestamp = ''\n",
        "                for string in event.findall(tags['string'], ns):\n",
        "                    if string.attrib['key'] == 'concept:name':\n",
        "                        task = string.attrib['value']                        \n",
        "                    if string.attrib['key'] == 'org:resource':\n",
        "                        user = string.attrib['value']\n",
        "                    if string.attrib['key'] == 'lifecycle:transition':\n",
        "                        event_type = string.attrib['value'].lower()\n",
        "                    if string.attrib['key'] == 'Complete_Timestamp':\n",
        "                        complete_timestamp = string.attrib['value']\n",
        "                        if complete_timestamp != 'End':\n",
        "                            complete_timestamp = datetime.datetime.strptime(complete_timestamp, end_timeformat)\n",
        "                timestamp = ''\n",
        "                for date in event.findall(tags['date'], ns):\n",
        "                    if date.attrib['key'] == 'time:timestamp':\n",
        "                        timestamp = date.attrib['value']\n",
        "                        try:\n",
        "                            timestamp = datetime.datetime.strptime(timestamp[:-6], start_timeformat)\n",
        "                        except ValueError:\n",
        "                            timestamp = datetime.datetime.strptime(timestamp, start_timeformat)\n",
        "                if not (task == '0' or task == '-1'):\n",
        "                    temp_data.append(\n",
        "                        dict(caseid=caseid, task=task, event_type=event_type, user=user, start_timestamp=timestamp,\n",
        "                             end_timestamp=complete_timestamp))\n",
        "            i += 1\n",
        "        raw_data = temp_data\n",
        "        temp_data = self.reorder_xes(temp_data, one_timestamp)\n",
        "        print_done_task()\n",
        "        return temp_data, raw_data\n",
        "\n",
        "    def reorder_xes(self, temp_data, one_timestamp):\n",
        "        \"\"\"this method joints the duplicated events on the .xes log\"\"\"\n",
        "        ordered_event_log = list()\n",
        "        if one_timestamp:\n",
        "            ordered_event_log = list(filter(lambda x: x['event_type'] == 'complete', temp_data))\n",
        "            for event in ordered_event_log:\n",
        "                event['end_timestamp'] = event['start_timestamp']\n",
        "        else:\n",
        "            events = list(filter(lambda x: (x['event_type'] == 'start' or x['event_type'] == 'complete'), temp_data))\n",
        "            cases = list({x['caseid'] for x in events})\n",
        "            for case in cases:\n",
        "                start_events = sorted(list(filter(lambda x: x['event_type'] == 'start' and x['caseid'] == case, events)), key=lambda x:x['start_timestamp'])\n",
        "                finish_events = sorted(list(filter(lambda x: x['event_type'] == 'complete' and x['caseid'] == case, events)), key=lambda x:x['start_timestamp'])\n",
        "                if len(start_events) == len(finish_events):\n",
        "                    temp_trace = list()\n",
        "                    for i, _ in enumerate(start_events):\n",
        "                        match = False\n",
        "                        for j, _ in enumerate(finish_events):\n",
        "                            if start_events[i]['task'] == finish_events[j]['task']:\n",
        "                                temp_trace.append(dict(caseid=case, task=start_events[i]['task'], event_type=start_events[i]['task'],\n",
        "                                     user=start_events[i]['user'], start_timestamp=start_events[i]['start_timestamp'], end_timestamp=finish_events[j]['start_timestamp']))\n",
        "                                match = True\n",
        "                                break\n",
        "                        if match:\n",
        "                            del finish_events[j]\n",
        "                    if match:\n",
        "                        ordered_event_log.extend(temp_trace)\n",
        "        return ordered_event_log\n",
        "\n",
        "    def get_mxml_events_data(self, filename,start_timeformat, end_timeformat):\n",
        "        \"\"\"read and parse all the events information from a MXML file\"\"\"\n",
        "        temp_data = list()\n",
        "        tree = ET.parse(filename)\n",
        "        root = tree.getroot()\n",
        "        process = root.find('Process')\n",
        "        procInstas = process.findall('ProcessInstance')\n",
        "        i = 0\n",
        "        for procIns in procInstas:\n",
        "            print_progress(((i / (len(procInstas) - 1)) * 100), 'Reading log traces ')\n",
        "            caseid = procIns.get('id')\n",
        "            complete_timestamp = ''\n",
        "            auditTrail = procIns.findall('AuditTrailEntry')\n",
        "            for trail in auditTrail:\n",
        "                task = ''\n",
        "                user = ''\n",
        "                event_type = ''\n",
        "                type_task = ''\n",
        "                timestamp = ''\n",
        "                attributes = trail.find('Data').findall('Attribute')\n",
        "                for attr in attributes:\n",
        "                    if (attr.get('name') == 'concept:name'):\n",
        "                        task = attr.text\n",
        "                    if (attr.get('name') == 'lifecycle:transition'):\n",
        "                        event_type = attr.text\n",
        "                    if (attr.get('name') == 'org:resource'):\n",
        "                        user = attr.text\n",
        "                    if (attr.get('name') == 'type_task'):\n",
        "                        type_task = attr.text\n",
        "                work_flow_ele = trail.find('WorkflowModelElement').text\n",
        "                event_type = trail.find('EventType').text\n",
        "                timestamp = trail.find('Timestamp').text\n",
        "                originator = trail.find('Originator').text\n",
        "                timestamp = datetime.datetime.strptime(trail.find('Timestamp').text[:-6], start_timeformat)\n",
        "                temp_data.append(\n",
        "                    dict(caseid=caseid, task=task, event_type=event_type, user=user, start_timestamp=timestamp,\n",
        "                         end_timestamp=timestamp))\n",
        "\n",
        "            i += 1\n",
        "        raw_data = temp_data\n",
        "        temp_data = self.reorder_mxml(temp_data)\n",
        "        print_done_task()\n",
        "        return temp_data, raw_data\n",
        "\n",
        "    def reorder_mxml(self, temp_data):\n",
        "        \"\"\"this method joints the duplicated events on the .mxml log\"\"\"\n",
        "        data = list()\n",
        "        start_events = list(filter(lambda x: x['event_type'] == 'start', temp_data))\n",
        "        finish_events = list(filter(lambda x: x['event_type'] == 'complete', temp_data))\n",
        "        for x, y in zip(start_events, finish_events):\n",
        "            data.append(dict(caseid=x['caseid'], task=x['task'], event_type=x['event_type'],\n",
        "                             user=x['user'], start_timestamp=x['start_timestamp'], end_timestamp=y['start_timestamp']))\n",
        "        return data\n",
        "\n",
        "    def get_csv_events_data(self, log_columns_numbers, start_timeformat, end_timeformat):\n",
        "        \"\"\"reads and parse all the events information from a csv file\"\"\"\n",
        "        flength = file_size(self.input)\n",
        "        i = 0\n",
        "        temp_data = list()\n",
        "        with open(self.input, 'r') as csvfile:\n",
        "            filereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "            next(filereader, None)  # skip the headers\n",
        "            for row in filereader:\n",
        "                print_progress(((i / (flength - 1)) * 100), 'Reading log traces ')\n",
        "                timestamp = ''\n",
        "                complete_timestamp = ''\n",
        "                if row[log_columns_numbers[1]] != 'End':\n",
        "                    timestamp = datetime.datetime.strptime(row[log_columns_numbers[4]], start_timeformat)\n",
        "                    complete_timestamp = datetime.datetime.strptime(row[log_columns_numbers[5]], end_timeformat)\n",
        "                temp_data.append(dict(caseid=row[log_columns_numbers[0]], task=row[log_columns_numbers[1]],\n",
        "                                      event_type=row[log_columns_numbers[2]], user=row[log_columns_numbers[3]],\n",
        "                                      start_timestamp=timestamp, end_timestamp=complete_timestamp))\n",
        "                i += 1\n",
        "        return temp_data, temp_data\n",
        "\n",
        "    # TODO manejo de excepciones\n",
        "    def find_first_task(self):\n",
        "        \"\"\"finds the first task\"\"\"\n",
        "        cases = list()\n",
        "        [cases.append(c['caseid']) for c in self.data]\n",
        "        cases = sorted(list(set(cases)))\n",
        "        first_task_names = list()\n",
        "        for case in cases:\n",
        "            trace = sorted(list(filter(lambda x: (x['caseid'] == case), self.data)), key=itemgetter('start_timestamp'))\n",
        "            first_task_names.append(trace[0]['task'])\n",
        "        first_task_names = list(set(first_task_names))\n",
        "        return first_task_names\n",
        "\n",
        "    def get_traces(self):\n",
        "        \"\"\"returns the data splitted by caseid and ordered by start_timestamp\"\"\"\n",
        "        cases = list()\n",
        "        for c in self.data: cases.append(c['caseid'])\n",
        "        cases = sorted(list(set(cases)))\n",
        "        traces = list()\n",
        "        for case in cases:\n",
        "            # trace = sorted(list(filter(lambda x: (x['caseid'] == case), self.data)), key=itemgetter('start_timestamp'))\n",
        "            trace = list(filter(lambda x: (x['caseid'] == case), self.data))\n",
        "            traces.append(trace)\n",
        "        return traces\n",
        "\n",
        "    def get_raw_traces(self):\n",
        "        \"\"\"returns the raw data splitted by caseid and ordered by start_timestamp\"\"\"\n",
        "        cases = list()\n",
        "        for c in self.raw_data: cases.append(c['caseid'])\n",
        "        cases = sorted(list(set(cases)))\n",
        "        traces = list()\n",
        "        for case in cases:\n",
        "            trace = sorted(list(filter(lambda x: (x['caseid'] == case), self.raw_data)), key=itemgetter('start_timestamp'))\n",
        "            traces.append(trace)\n",
        "        return traces\n",
        "\n",
        "    def read_resource_task(self,task,roles):\n",
        "        \"\"\"returns the resource that performs a task\"\"\"\n",
        "        filtered_list = list(filter(lambda x: x['task']==task, self.data))\n",
        "        role_assignment = list()\n",
        "        for task in filtered_list:\n",
        "            for role in roles:\n",
        "                for member in role['members']:\n",
        "                    if task['user']==member:\n",
        "                        role_assignment.append(role['role'])\n",
        "        return max(role_assignment)\n",
        "\n",
        "    def set_data(self,data):\n",
        "        \"\"\"seting method for the data attribute\"\"\"\n",
        "        self.data = data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugAaRURhByTl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Model to predict next activity using attention (prefix attention only)\n",
        "A model that predicts next activity using attention. The contribution of each prefix in the prediction is useful to focus on the time steps that contribute the most to the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytyfMru8magq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.layers as L\n",
        "from keras import backend as K\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.layers import Lambda, dot, Activation, concatenate, Dense\n",
        "\n",
        "\n",
        "def training_model_temporal(vec, ac_weights, rl_weights, output_folder, args):\n",
        " \n",
        "  MAX_LEN = args['n_size']\n",
        "  dropout_input = 0.15\n",
        "  dropout_context=0.15\n",
        "  # number of lstm cells\n",
        "  incl_time = True \n",
        "  incl_res = True\n",
        "  lstm_size_alpha=args['l_size']\n",
        "  print(\"Training prefix-attention model\")\n",
        "\n",
        "  l2reg=0.0001\n",
        "\n",
        " \n",
        "  #Inputs include activity, resource and time - time is normalised- 0 mean and unit variance\n",
        "  ac_input = Input(shape=(vec['prefixes']['x_ac_inp'].shape[1], ), name='ac_input')\n",
        "  rl_input = Input(shape=(vec['prefixes']['x_rl_inp'].shape[1], ), name='rl_input')\n",
        "  t_input = Input(shape=(vec['prefixes']['xt_inp'].shape[1], 1), name='t_input')\n",
        "  \n",
        "  ac_embedding = L.Embedding(ac_weights.shape[0],\n",
        "                            ac_weights.shape[1],\n",
        "                            weights=[ac_weights],\n",
        "                            input_length=vec['prefixes']['x_ac_inp'].shape[1],\n",
        "                            trainable=True, name='ac_embedding')(ac_input)\n",
        "\n",
        "  dim =ac_weights.shape[1]   \n",
        "\n",
        "  if incl_res:\n",
        "      rl_embedding = Embedding(rl_weights.shape[0],\n",
        "                            rl_weights.shape[1],\n",
        "                            weights=[rl_weights],\n",
        "                            input_length=vec['prefixes']['x_rl_inp'].shape[1],\n",
        "                            trainable=True, name='rl_embedding')(rl_input)\n",
        "      full_embs = L.concatenate([ac_embedding, rl_embedding], name='catInp')\n",
        "      dim += rl_weights.shape[1]\n",
        "      \n",
        "  else:\n",
        "      full_embs = ac_embedding\n",
        "\n",
        "        #Apply dropout on inputs\n",
        "  full_embs = L.Dropout(dropout_input)(full_embs)\n",
        "    \n",
        "  if incl_time==True:\n",
        "      time_embs = L.concatenate([full_embs, t_input], name='allInp')\n",
        "      \n",
        "      dim += 1\n",
        "  else:\n",
        "        time_embs=full_embs\n",
        "\n",
        "  alpha = L.Bidirectional(L.CuDNNLSTM(lstm_size_alpha, return_sequences=True),\n",
        "                                    name='alpha')\n",
        "  alpha_dense = L.Dense(1, kernel_regularizer=l2(l2reg))\n",
        " \n",
        "  #Compute alpha, timestep attention\n",
        "  alpha_out = alpha(time_embs)\n",
        "  alpha_out = L.TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n",
        "  alpha_out = L.Softmax(name='alpha_softmax', axis=1)(alpha_out)\n",
        "  \n",
        "  #Compute context vector based on attentions and embeddings\n",
        "  c_t = L.Multiply()([alpha_out, time_embs])\n",
        "  c_t = L.Lambda(lambda x: K.sum(x, axis=1))(c_t)\n",
        "  \n",
        "  contexts = L.Dropout(dropout_context)(c_t)\n",
        " \n",
        "  \n",
        "  act_output = Dense(ac_weights.shape[1],\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       name='act_output')(contexts)\n",
        "\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[ac_input, rl_input, t_input], outputs=act_output)\n",
        "\n",
        "  if args['optim'] == 'Nadam':\n",
        "        opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999,\n",
        "                    epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
        "  elif args['optim'] == 'Adam':\n",
        "        opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999,\n",
        "                   epsilon=None, decay=0.0, amsgrad=False)\n",
        "  elif args['optim'] == 'SGD':\n",
        "        opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "  elif args['optim'] == 'Adagrad':\n",
        "        opt = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "\n",
        "  model.compile(loss={'act_output':'categorical_crossentropy'}, optimizer=opt, metrics=['accuracy'])\n",
        "    \n",
        "  model.summary()\n",
        "    \n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
        "#\n",
        "#    # Output file\n",
        "  output_file_path = os.path.join(output_folder,\n",
        "                                    'models/model_rd_' + str(args['n_size']) +\n",
        "                                    ' ' + args['optim']  + args['log_name']  +\n",
        "                                    '_{epoch:02d}-{val_loss:.2f}.h5')\n",
        "  print('This is the output file path ', output_file_path)\n",
        "    # Saving\n",
        "  model_checkpoint = ModelCheckpoint(output_file_path,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=False,\n",
        "                                       mode='auto')\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.5,\n",
        "                                   patience=10,\n",
        "                                   verbose=0,\n",
        "                                   mode='auto',\n",
        "                                   min_delta=0.0001,\n",
        "                                   cooldown=0,\n",
        "                                   min_lr=0)\n",
        "  model_inputs = [vec['prefixes']['x_ac_inp']]\n",
        "  model_inputs.append(vec['prefixes']['x_rl_inp'])\n",
        "  model_inputs.append(vec['prefixes']['xt_inp'])\n",
        "\n",
        " #model.fit({'ac_input':, 'rl_input':, 't_input':},\n",
        "  model.fit(model_inputs,\n",
        "              {'act_output':vec['next_evt']['y_ac_inp']},\n",
        "              validation_split=0.15,\n",
        "              verbose=2,\n",
        "              callbacks=[early_stopping, model_checkpoint, lr_reducer],\n",
        "              batch_size=100,\n",
        "              epochs=100)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjaA74JyRxy4",
        "colab_type": "text"
      },
      "source": [
        "#### Model - Next activity prediction using attention (prefix & feature)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqOW_Uzq128i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.layers as L\n",
        "from keras import backend as K\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.layers import Lambda, dot, Activation, concatenate, Dense\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_model_temporal_variable(vec, ac_weights, rl_weights, output_folder, args):\n",
        "\n",
        "  \n",
        "  dropout_input = 0.01\n",
        "  dropout_context=0.30\n",
        "  lstm_size_alpha=args['l_size']\n",
        "  lstm_size_beta=args['l_size']\n",
        "  print(\"Training prefix and variable attention model\")\n",
        "\n",
        "  l2reg=0.0001\n",
        "  allow_negative=False\n",
        "  incl_time = True \n",
        "  incl_res = True\n",
        "        #Code Input\n",
        "  ac_input = Input(shape=(vec['prefixes']['x_ac_inp'].shape[1], ), name='ac_input')\n",
        "  rl_input = Input(shape=(vec['prefixes']['x_rl_inp'].shape[1], ), name='rl_input')\n",
        "  t_input = Input(shape=(vec['prefixes']['xt_inp'].shape[1], 1), name='t_input')\n",
        "\n",
        " \n",
        "\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "  #inputs_list = [ac_input]\n",
        "   \n",
        "        #Calculate embedding for each code and sum them to a visit level\n",
        "  ac_embedding = L.Embedding(ac_weights.shape[0],\n",
        "                            ac_weights.shape[1],\n",
        "                            weights=[ac_weights],\n",
        "                            input_length=vec['prefixes']['x_ac_inp'].shape[1],\n",
        "                            trainable=True, name='ac_embedding')(ac_input)\n",
        "\n",
        "  dim =ac_weights.shape[1]   \n",
        "  \n",
        "  if incl_res:\n",
        "      rl_embedding = Embedding(rl_weights.shape[0],\n",
        "                            rl_weights.shape[1],\n",
        "                            weights=[rl_weights],\n",
        "                            input_length=vec['prefixes']['x_rl_inp'].shape[1],\n",
        "                            trainable=True, name='rl_embedding')(rl_input)\n",
        "      full_embs = L.concatenate([ac_embedding, rl_embedding], name='catInp')\n",
        "      dim += rl_weights.shape[1]\n",
        "      \n",
        "  else:\n",
        "      full_embs = ac_embedding\n",
        "\n",
        "        #Apply dropout on inputs\n",
        "  full_embs = L.Dropout(dropout_input)(full_embs)\n",
        "    \n",
        "  if incl_time==True:\n",
        "      time_embs = L.concatenate([full_embs, t_input], name='allInp')\n",
        "      \n",
        "      dim += 1\n",
        "  else:\n",
        "        time_embs=full_embs\n",
        "    \n",
        "      #Numeric input if needed\n",
        "  alpha = L.Bidirectional(L.CuDNNLSTM(lstm_size_alpha, return_sequences=True),\n",
        "                                    name='alpha')\n",
        "  beta = L.Bidirectional(L.CuDNNLSTM(lstm_size_beta, return_sequences=True),\n",
        "                                   name='beta')\n",
        "  alpha_dense = L.Dense(1, kernel_regularizer=l2(l2reg))\n",
        "  beta_dense = L.Dense(dim,\n",
        "                             activation='tanh', kernel_regularizer=l2(l2reg))\n",
        "\n",
        "  #Compute alpha, visit attention\n",
        "  alpha_out = alpha(time_embs)\n",
        "  alpha_out = L.TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n",
        "  alpha_out = L.Softmax(axis=1, name='alpha_softmax')(alpha_out)\n",
        "  #Compute beta, codes attention\n",
        "  beta_out = beta(time_embs)\n",
        "  beta_out = L.TimeDistributed(beta_dense, name='beta_dense_0')(beta_out)\n",
        "  #Compute context vector based on attentions and embeddings\n",
        "  c_t = L.Multiply()([alpha_out, beta_out, time_embs])\n",
        "  c_t = L.Lambda(lambda x: K.sum(x, axis=1))(c_t)\n",
        "        #Reshape to 3d vector for consistency between Many to Many and Many to One implementations\n",
        "        #contexts = L.Lambda(reshape)(c_t)\n",
        "\n",
        "  #Make a prediction\n",
        "  contexts = L.Dropout(dropout_context)(c_t)\n",
        " \n",
        "  act_output = Dense(ac_weights.shape[0],\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       name='act_output')(contexts)\n",
        "\n",
        "    \n",
        "  model = Model(inputs=[ac_input, rl_input, t_input], outputs=act_output)\n",
        "\n",
        "  if args['optim'] == 'Nadam':\n",
        "        opt = Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999,\n",
        "                    epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
        "  elif args['optim'] == 'Adam':\n",
        "        opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999,\n",
        "                   epsilon=None, decay=0.0, amsgrad=False)\n",
        "  elif args['optim'] == 'SGD':\n",
        "        opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "  elif args['optim'] == 'Adagrad':\n",
        "        opt = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "\n",
        "  model.compile(loss={'act_output':'categorical_crossentropy'}, optimizer=opt)\n",
        "    \n",
        "  model.summary()\n",
        "    \n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
        "#\n",
        "#    # Output file\n",
        "  output_file_path = os.path.join(output_folder,\n",
        "                                    'models/model_rd_' + str(args['n_size']) +\n",
        "                                    ' ' + args['optim']  + args['log_name']  +\n",
        "                                    '_{epoch:02d}-{val_loss:.2f}.h5')\n",
        "  print('This is the output file path ', output_file_path)\n",
        "    # Saving\n",
        "  model_checkpoint = ModelCheckpoint(output_file_path,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=False,\n",
        "                                       mode='auto')\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.5,\n",
        "                                   patience=10,\n",
        "                                   verbose=0,\n",
        "                                   mode='auto',\n",
        "                                   min_delta=0.0001,\n",
        "                                   cooldown=0,\n",
        "                                   min_lr=0)\n",
        "  model_inputs = [vec['prefixes']['x_ac_inp']]\n",
        "  model_inputs.append(vec['prefixes']['x_rl_inp'])\n",
        "  model_inputs.append(vec['prefixes']['xt_inp'])\n",
        "\n",
        " #model.fit({'ac_input':, 'rl_input':, 't_input':},\n",
        "  model.fit(model_inputs,\n",
        "              {'act_output':vec['next_evt']['y_ac_inp']},\n",
        "              validation_split=0.2,\n",
        "              verbose=2,\n",
        "              callbacks=[early_stopping, model_checkpoint, lr_reducer],\n",
        "              batch_size=50,\n",
        "              epochs=100)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSQwdZH7T8hL",
        "colab_type": "text"
      },
      "source": [
        "#### Model - Next activity and time prediction using attention (prefix & feature)\n",
        "The model is use to predict remaining time and next activity - this can be used to see the dependence between these learning models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ4T5_VWT9FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_model_with_time_prediction(vec, ac_weights, rl_weights, output_folder, args):\n",
        "\n",
        "\n",
        "  dropout_input = 0.01\n",
        "  dropout_context=0.35\n",
        "  lstm_size_alpha=args['l_size']\n",
        "  lstm_size_beta=args['l_size']\n",
        "  print(\"Training activity, time and role with attention\")\n",
        "  l2reg=0.0005\n",
        "  allow_negative=False\n",
        "  incl_time = True \n",
        "  incl_res = True\n",
        "  ac_input = Input(shape=(vec['prefixes']['x_ac_inp'].shape[1], ), name='ac_input')\n",
        "  rl_input = Input(shape=(vec['prefixes']['x_rl_inp'].shape[1], ), name='rl_input')\n",
        "  t_input = Input(shape=(vec['prefixes']['xt_inp'].shape[1], 1), name='t_input')\n",
        "\n",
        " \n",
        "        #Calculate embedding for each code and sum them to a visit level\n",
        "  ac_embedding = L.Embedding(ac_weights.shape[0],\n",
        "                            ac_weights.shape[1],\n",
        "                            weights=[ac_weights],\n",
        "                            input_length=vec['prefixes']['x_ac_inp'].shape[1],\n",
        "                            trainable=True, name='ac_embedding')(ac_input)\n",
        "\n",
        "  dim =ac_weights.shape[1]   \n",
        "  if incl_res:\n",
        "      rl_embedding = Embedding(rl_weights.shape[0],\n",
        "                            rl_weights.shape[1],\n",
        "                            weights=[rl_weights],\n",
        "                            input_length=vec['prefixes']['x_rl_inp'].shape[1],\n",
        "                            trainable=True, name='rl_embedding')(rl_input)\n",
        "      full_embs = L.concatenate([ac_embedding, rl_embedding], name='catInp')\n",
        "      dim += rl_weights.shape[1]\n",
        "  else:\n",
        "      full_embs = ac_embedding\n",
        "\n",
        "        #Apply dropout on inputs\n",
        "  full_embs = L.Dropout(dropout_input)(full_embs)\n",
        "    \n",
        "  if incl_time==True:\n",
        "      time_embs = L.concatenate([full_embs, t_input], name='allInp')\n",
        "      #input_list.append(t_input)\n",
        "      dim += 1\n",
        "  else:\n",
        "        time_embs=full_embs\n",
        "\n",
        "\n",
        "\n",
        "  #Numeric input if needed\n",
        "  alpha = L.Bidirectional(L.LSTM(lstm_size_alpha, return_sequences=True),\n",
        "                                    name='alpha')\n",
        "  beta = L.Bidirectional(L.LSTM(lstm_size_beta, return_sequences=True),\n",
        "                                   name='beta')\n",
        "  alpha_dense = L.Dense(1, kernel_regularizer=l2(l2reg))\n",
        "  beta_dense = L.Dense(dim,\n",
        "                             activation='tanh', kernel_regularizer=l2(l2reg))\n",
        "\n",
        "  #Compute alpha, visit attention\n",
        "  alpha_out = alpha(time_embs)\n",
        "  alpha_out = L.TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n",
        "  alpha_out = L.Softmax(axis=1, name='alpha_softmax')(alpha_out)\n",
        "  #Compute beta, codes attention\n",
        "  beta_out = beta(time_embs)\n",
        "  beta_out = L.TimeDistributed(beta_dense, name='beta_dense_0')(beta_out)\n",
        "  #Compute context vector based on attentions and embeddings\n",
        "  c_t = L.Multiply()([alpha_out, beta_out, time_embs])\n",
        "  c_t = L.Lambda(lambda x: K.sum(x, axis=1))(c_t)\n",
        "  #Reshape to 3d vector for consistency between Many to Many and Many to One implementations\n",
        "  #contexts = L.Lambda(reshape)(c_t)\n",
        "\n",
        "  #Make a prediction\n",
        "  contexts = L.Dropout(dropout_context)(c_t)\n",
        " \n",
        "  #batch1 = BatchNormalization()(contexts)\n",
        "\n",
        "\n",
        "  act_output = Dense(ac_weights.shape[0],\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       name='act_output')(contexts)\n",
        "\n",
        "  rl_output = Dense(rl_weights.shape[0],\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       name='rl_output')(contexts)\n",
        "\n",
        "\n",
        "  #t1_output = Dense(10, kernel_initializer='glorot_uniform',\n",
        "  #                     name='tin_output')(contexts)\n",
        "\n",
        "\n",
        "\n",
        "  t_output = Dense(1, kernel_initializer='glorot_uniform',\n",
        "                       name='t_output')(contexts)\n",
        " \n",
        "  model = Model(inputs=[ac_input, rl_input, t_input], outputs=[act_output, rl_output, t_output])\n",
        "\n",
        "  if args['optim'] == 'Nadam':\n",
        "      opt = Nadam(lr=0.001, beta_1=0.9, beta_2=0.999,\n",
        "                    epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
        "  elif args['optim'] == 'Adam':\n",
        "      opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999,\n",
        "                   epsilon=None, decay=0.0, amsgrad=False)\n",
        "  elif args['optim'] == 'SGD':\n",
        "      opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "  elif args['optim'] == 'Adagrad':\n",
        "      opt = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "\n",
        "  model.compile(loss={'t_output':'mae','act_output':'categorical_crossentropy', 'rl_output':'categorical_crossentropy'}, \n",
        "                optimizer=opt, metrics=['accuracy','mae'])\n",
        "    \n",
        "  model.summary()\n",
        "    \n",
        "  early_stopping = EarlyStopping(monitor='val_loss',verbose=1, patience=42)\n",
        "#\n",
        "#    # Output file\n",
        "  output_file_path = os.path.join(output_folder,\n",
        "                                    'models/model_rd_' + str(args['l_size']) +\n",
        "                                    ' ' + args['optim'] +  args['log_name'] +\n",
        "                                    '_{epoch:02d}-{val_loss:.2f}.h5')\n",
        "  print('This is the output file path ', output_file_path)\n",
        "    # Saving\n",
        "  model_checkpoint = ModelCheckpoint(output_file_path,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=False,\n",
        "                                       mode='auto')\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.5,\n",
        "                                   patience=10,\n",
        "                                   verbose=0,\n",
        "                                   mode='auto',\n",
        "                                   min_delta=0.0001,\n",
        "                                   cooldown=0,\n",
        "                                   min_lr=0)\n",
        "\n",
        "  model.fit({'ac_input':vec['prefixes']['x_ac_inp'],\n",
        "               'rl_input':vec['prefixes']['x_rl_inp'],\n",
        "               't_input':vec['prefixes']['xt_inp']},\n",
        "              {'act_output':vec['next_evt']['y_ac_inp'],\n",
        "               'rl_output':vec['next_evt']['y_rl_inp'],\n",
        "               't_output':vec['next_evt']['yt_inp']},\n",
        "              validation_split=0.15,\n",
        "              verbose=0,\n",
        "              callbacks=[early_stopping, model_checkpoint, lr_reducer],\n",
        "              batch_size=50,\n",
        "              epochs=100)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83SmskGB_GG",
        "colab_type": "text"
      },
      "source": [
        "####Model Training (previous work + changes to feature representations)\n",
        "\n",
        "Use the same preprocessing apprpoach as done by previous works. This work uses different approach to normalisation of time (zero mean and unit variance). The encoding of activity and roles is binary encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZWOvyUnb-WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Nov 21 21:23:55 2018\n",
        "\n",
        "@author: Manuel Camargo\n",
        "\"\"\"\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "import keras.utils as ku\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def training_model(timeformat, args, no_loops=False):\n",
        "    \"\"\"Main method of the training module.\n",
        "    Args:\n",
        "        timeformat (str): event-log date-time format.\n",
        "        args (dict): parameters for training the network.\n",
        "        no_loops (boolean): remove loops fom the event-log (optional).\n",
        "    \"\"\"\n",
        "    parameters = dict()\n",
        "    # read the logfile\n",
        "    log = LogReader( args['file_name'],\n",
        "                       timeformat, timeformat, one_timestamp=True)\n",
        "    _, resource_table = read_resource_pool(log, sim_percentage=0.50)\n",
        "    # Role discovery\n",
        "    log_df_resources = pd.DataFrame.from_records(resource_table)\n",
        "    log_df_resources = log_df_resources.rename(index=str, columns={\"resource\": \"user\"})\n",
        "    # Dataframe creation\n",
        "    log_df = pd.DataFrame.from_records(log.data)\n",
        "    log_df = log_df.merge(log_df_resources, on='user', how='left')\n",
        "    log_df = log_df[log_df.task != 'Start']\n",
        "    log_df = log_df[log_df.task != 'End']\n",
        "    log_df = log_df.reset_index(drop=True)\n",
        "\n",
        "    if no_loops:\n",
        "        log_df = reduce_loops(log_df)\n",
        "    # Index creation\n",
        "    ac_index = create_index(log_df, 'task')\n",
        "    ac_index['start'] = 0\n",
        "    ac_index['end'] = len(ac_index)\n",
        "    index_ac = {v: k for k, v in ac_index.items()}\n",
        "\n",
        "    rl_index = create_index(log_df, 'role')\n",
        "    rl_index['start'] = 0\n",
        "    rl_index['end'] = len(rl_index)\n",
        "    index_rl = {v: k for k, v in rl_index.items()}\n",
        "\n",
        "    # Load embedded matrix\n",
        "    ac_weights = ku.to_categorical(sorted(index_ac.keys()), len(ac_index))\n",
        "    print('AC_WEIGHTS', ac_weights)\n",
        "    rl_weights =  ku.to_categorical(sorted(index_rl.keys()), len(rl_index))\n",
        "    print('RL_WEIGHTS', rl_weights)\n",
        "\n",
        "\n",
        "    # Calculate relative times\n",
        "    log_df = add_calculated_features(log_df, ac_index, rl_index)\n",
        "    # Split validation datasets\n",
        "    log_df_train, log_df_test = split_train_test(log_df, 0.3) # 70%/30%\n",
        "    # Input vectorization\n",
        "    vec = vectorization(log_df_train, ac_index, rl_index, args)\n",
        "    #print(vec['prefixes']['x_ac_inp'])\n",
        "    \n",
        "    # Parameters export\n",
        "    output_folder = os.path.join(args['folder'])\n",
        "    print('Passing output_folder======', output_folder)\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "        os.makedirs(os.path.join(output_folder, 'parameters'))\n",
        "\n",
        "    parameters['event_log'] = args['file_name']\n",
        "    parameters['exp_desc'] = args\n",
        "    parameters['index_ac'] = index_ac\n",
        "    parameters['index_rl'] = index_rl\n",
        "    parameters['dim'] = dict(samples=str(vec['prefixes']['x_ac_inp'].shape[0]),\n",
        "                             time_dim=str(vec['prefixes']['x_ac_inp'].shape[1]),\n",
        "                             features=str(len(ac_index)))\n",
        "    parameters['mean_tbtw'] = vec['mean_tbtw']\n",
        "    parameters['std_tbtw'] = vec['std_tbtw']\n",
        "\n",
        "    create_json(parameters, os.path.join(output_folder,\n",
        "                                             'parameters', \n",
        "                                            args['log_name']+'model_parameters.json'))\n",
        "    \n",
        "    #pickle.dump(vec, open( os.path.join(output_folder,\n",
        "    #                                        'parameters', \n",
        "    #                                    args['log_name']+'train_vec.pkl'), \"wb\"))\n",
        "    \n",
        "    \n",
        "    create_csv_file_header(log_df_test.to_dict('records'),\n",
        "                               os.path.join(output_folder, \n",
        "                                            'parameters', \n",
        "                                            args['log_name']+'test_log.csv'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if(args['task']=='prefix_attn'):\n",
        "        model = training_model_temporal(vec, ac_weights, rl_weights, output_folder, args)\n",
        "    elif(args['task']=='full_attn'):\n",
        "        model = training_model_temporal_variable(vec, ac_weights, rl_weights, output_folder, args)\n",
        "    else:\n",
        "        model = training_model_with_time_prediction(vec, ac_weights, rl_weights, output_folder, args)\n",
        "  \n",
        "    #elif args['model_type'] == 'shared_cat':\n",
        "    #    training_model_sharedcat(vec, ac_weights, rl_weights, output_folder, args)\n",
        "    \n",
        "   \n",
        "# =============================================================================\n",
        "# Pre-processing: n-gram vectorization\n",
        "# =============================================================================\n",
        "def vectorization(log_df, ac_index, rl_index, args):\n",
        "    \"\"\"Example function with types documented in the docstring.\n",
        "    Args:\n",
        "        log_df (dataframe): event log data.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "        args (dict): parameters for training the network\n",
        "    Returns:\n",
        "        dict: Dictionary that contains all the LSTM inputs.\n",
        "    \"\"\"\n",
        "    if args['norm_method'] == 'max':\n",
        "        mean_tbtw = np.mean(log_df.tbtw)\n",
        "        std_tbtw = np.std(log_df.tbtw)\n",
        "        norm = lambda x: (x['tbtw']-mean_tbtw)/std_tbtw\n",
        "        log_df['tbtw_norm'] = log_df.apply(norm, axis=1)\n",
        "        log_df = reformat_events(log_df, ac_index, rl_index)\n",
        "    elif args['norm_method'] == 'lognorm':\n",
        "        logit = lambda x: math.log1p(x['tbtw'])\n",
        "        log_df['tbtw_log'] = log_df.apply(logit, axis=1)\n",
        "        mean_tbtw = np.mean(log_df.tbtw_log)\n",
        "        std_tbtw=np.std(log_df.tbtw_log)\n",
        "        norm = lambda x: (x['tbtw_log']-mean_tbtw)/std_tbtw\n",
        "        log_df['tbtw_norm'] = log_df.apply(norm, axis=1)\n",
        "        log_df = reformat_events(log_df, ac_index, rl_index)\n",
        "\n",
        "    vec = {'prefixes':dict(), 'next_evt':dict(), 'mean_tbtw':mean_tbtw, 'std_tbtw':std_tbtw}\n",
        "    # n-gram definition\n",
        "    for i, _ in enumerate(log_df):\n",
        "        ac_n_grams = list(ngrams(log_df[i]['ac_order'], args['n_size'],\n",
        "                                 pad_left=True, left_pad_symbol=0))\n",
        "        rl_n_grams = list(ngrams(log_df[i]['rl_order'], args['n_size'],\n",
        "                                 pad_left=True, left_pad_symbol=0))\n",
        "        tn_grams = list(ngrams(log_df[i]['tbtw'], args['n_size'],\n",
        "                               pad_left=True, left_pad_symbol=0))\n",
        "        st_idx = 0\n",
        "        if i == 0:\n",
        "            vec['prefixes']['x_ac_inp'] = np.array([ac_n_grams[0]])\n",
        "            vec['prefixes']['x_rl_inp'] = np.array([rl_n_grams[0]])\n",
        "            vec['prefixes']['xt_inp'] = np.array([tn_grams[0]])\n",
        "            vec['next_evt']['y_ac_inp'] = np.array(ac_n_grams[1][-1])\n",
        "            vec['next_evt']['y_rl_inp'] = np.array(rl_n_grams[1][-1])\n",
        "            vec['next_evt']['yt_inp'] = np.array(tn_grams[1][-1])\n",
        "            st_idx = 1\n",
        "        for j in range(st_idx, len(ac_n_grams)-1):\n",
        "            vec['prefixes']['x_ac_inp'] = np.concatenate((vec['prefixes']['x_ac_inp'],\n",
        "                                                          np.array([ac_n_grams[j]])), axis=0)\n",
        "            vec['prefixes']['x_rl_inp'] = np.concatenate((vec['prefixes']['x_rl_inp'],\n",
        "                                                          np.array([rl_n_grams[j]])), axis=0)\n",
        "            vec['prefixes']['xt_inp'] = np.concatenate((vec['prefixes']['xt_inp'],\n",
        "                                                        np.array([tn_grams[j]])), axis=0)\n",
        "            vec['next_evt']['y_ac_inp'] = np.append(vec['next_evt']['y_ac_inp'],\n",
        "                                                    np.array(ac_n_grams[j+1][-1]))\n",
        "            vec['next_evt']['y_rl_inp'] = np.append(vec['next_evt']['y_rl_inp'],\n",
        "                                                    np.array(rl_n_grams[j+1][-1]))\n",
        "            vec['next_evt']['yt_inp'] = np.append(vec['next_evt']['yt_inp'],\n",
        "                                                  np.array(tn_grams[j+1][-1]))\n",
        "\n",
        "    vec['prefixes']['xt_inp'] = vec['prefixes']['xt_inp'].reshape(\n",
        "        (vec['prefixes']['xt_inp'].shape[0],\n",
        "         vec['prefixes']['xt_inp'].shape[1], 1))\n",
        "    \n",
        "   \n",
        "    \n",
        "    #print(vec['prefixes']['x_ac_inp'])\n",
        "    vec['next_evt']['y_ac_inp'] = ku.to_categorical(vec['next_evt']['y_ac_inp'],\n",
        "                                                    num_classes=len(ac_index))\n",
        "\n",
        "    vec['next_evt']['y_rl_inp'] = ku.to_categorical(vec['next_evt']['y_rl_inp'],\n",
        "                                                    num_classes=len(rl_index))\n",
        "    \n",
        "    return vec\n",
        "\n",
        "def add_calculated_features(log_df, ac_index, rl_index):\n",
        "    \"\"\"Appends the indexes and relative time to the dataframe.\n",
        "    Args:\n",
        "        log_df: dataframe.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "    Returns:\n",
        "        Dataframe: The dataframe with the calculated features added.\n",
        "    \"\"\"\n",
        "    ac_idx = lambda x: ac_index[x['task']]\n",
        "    log_df['ac_index'] = log_df.apply(ac_idx, axis=1)\n",
        "\n",
        "    rl_idx = lambda x: rl_index[x['role']]\n",
        "    log_df['rl_index'] = log_df.apply(rl_idx, axis=1)\n",
        "\n",
        "    log_df['tbtw'] = 0\n",
        "    log_df['tbtw_norm'] = 0\n",
        "\n",
        "    log_df = log_df.to_dict('records')\n",
        "\n",
        "    log_df = sorted(log_df, key=lambda x: (x['caseid'], x['end_timestamp']))\n",
        "    for _, group in itertools.groupby(log_df, key=lambda x: x['caseid']):\n",
        "        trace = list(group)\n",
        "        for i, _ in enumerate(trace):\n",
        "            if i != 0:\n",
        "                trace[i]['tbtw'] = (trace[i]['end_timestamp'] -\n",
        "                                    trace[i-1]['end_timestamp']).total_seconds()\n",
        "\n",
        "    return pd.DataFrame.from_records(log_df)\n",
        "\n",
        "def reformat_events(log_df, ac_index, rl_index):\n",
        "    \"\"\"Creates series of activities, roles and relative times per trace.\n",
        "    Args:\n",
        "        log_df: dataframe.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "    Returns:\n",
        "        list: lists of activities, roles and relative times.\n",
        "    \"\"\"\n",
        "    log_df = log_df.to_dict('records')\n",
        "\n",
        "    temp_data = list()\n",
        "    log_df = sorted(log_df, key=lambda x: (x['caseid'], x['end_timestamp']))\n",
        "    for key, group in itertools.groupby(log_df, key=lambda x: x['caseid']):\n",
        "        trace = list(group)\n",
        "        ac_order = [x['ac_index'] for x in trace]\n",
        "        rl_order = [x['rl_index'] for x in trace]\n",
        "        tbtw = [x['tbtw_norm'] for x in trace]\n",
        "        ac_order.insert(0, ac_index[('start')])\n",
        "        ac_order.append(ac_index[('end')])\n",
        "        rl_order.insert(0, rl_index[('start')])\n",
        "        rl_order.append(rl_index[('end')])\n",
        "        tbtw.insert(0, 0)\n",
        "        tbtw.append(0)\n",
        "        temp_dict = dict(caseid=key,\n",
        "                         ac_order=ac_order,\n",
        "                         rl_order=rl_order,\n",
        "                         tbtw=tbtw)\n",
        "        temp_data.append(temp_dict)\n",
        "\n",
        "    return temp_data\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Support\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def create_index(log_df, column):\n",
        "    \"\"\"Creates an idx for a categorical attribute.\n",
        "    Args:\n",
        "        log_df: dataframe.\n",
        "        column: column name.\n",
        "    Returns:\n",
        "        index of a categorical attribute pairs.\n",
        "    \"\"\"\n",
        "    temp_list = log_df[[column]].values.tolist()\n",
        "    subsec_set = {(x[0]) for x in temp_list}\n",
        "    subsec_set = sorted(list(subsec_set))\n",
        "    alias = dict()\n",
        "    for i, _ in enumerate(subsec_set):\n",
        "        alias[subsec_set[i]] = i + 1\n",
        "    return alias\n",
        "\n",
        "def max_serie(log_df, serie):\n",
        "    \"\"\"Returns the max and min value of a column.\n",
        "    Args:\n",
        "        log_df: dataframe.\n",
        "        serie: name of the serie.\n",
        "    Returns:\n",
        "        max and min value.\n",
        "    \"\"\"\n",
        "    max_value, min_value = 0, 0\n",
        "    for record in log_df:\n",
        "        if np.max(record[serie]) > max_value:\n",
        "            max_value = np.max(record[serie])\n",
        "        if np.min(record[serie]) > min_value:\n",
        "            min_value = np.min(record[serie])\n",
        "    return max_value, min_value\n",
        "\n",
        "def max_min_std(val, max_value, min_value):\n",
        "    \"\"\"Standardize a number between range.\n",
        "    Args:\n",
        "        val: Value to be standardized.\n",
        "        max_value: Maximum value of the range.\n",
        "        min_value: Minimum value of the range.\n",
        "    Returns:\n",
        "        Standardized value between 0 and 1.\n",
        "    \"\"\"\n",
        "    std = (val - min_value) / (max_value - min_value)\n",
        "    return std\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-BPW1f5Lqkt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Execute model training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPp4OxJKcC9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "MY_WORKSPACE_DIR = \"/content/drive/My Drive/BPIC_Data/\"\n",
        "\n",
        "\n",
        "#data_train_df = pd.read_pickle(MY_WORKSPACE_DIR +\"data_train.pkl\")\n",
        " \n",
        "import sys\n",
        "import getopt\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "\n",
        "def catch_parameter(opt):\n",
        "    \"\"\"Change the captured parameters names\"\"\"\n",
        "    switch = {'-h':'help', '-i':'imp', '-l':'lstm_act',\n",
        "              '-d':'dense_act', '-n':'norm_method', '-f':'folder',\n",
        "              '-m':'model_file', '-t':'model_type', '-a':'activity',\n",
        "              '-e':'file_name', '-b':'n_size', '-c':'l_size', '-o':'optim'}\n",
        "    try:\n",
        "        return switch[opt]\n",
        "    except:\n",
        "        raise Exception('Invalid option ' + opt)\n",
        "\n",
        "# --setup--\n",
        "def main():\n",
        "    \"\"\"Main aplication method\"\"\"\n",
        "    timeformat = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "    parameters = dict()\n",
        "#   Parameters setting manual fixed or catched by console for batch operations\n",
        "   \n",
        "    \n",
        "    parameters['folder'] = \"/content/drive/My Drive/BPIC_Data/output_files/\"\n",
        "#       Specific model training parameters\n",
        "    parameters['imp'] = 1 # keras lstm implementation 1 cpu, 2 gpu\n",
        "    parameters['lstm_act'] = None # optimization function see keras doc\n",
        "    parameters['dense_act'] = None # optimization function see keras doc\n",
        "    parameters['optim'] = 'Adagrad' # optimization function see keras doc\n",
        "    parameters['norm_method'] = 'lognorm' # max, lognorm\n",
        "                # Model types --> specialized, concatenated, shared_cat, joint, shared\n",
        "    parameters['model_type'] = 'shared_cat'\n",
        "    parameters['l_size'] = 50 # LSTM layer sizes\n",
        "#       Generation parameters\n",
        "    parameters['folder'] = \"/content/drive/My Drive/BPIC_Data/output_files/\"\n",
        "    parameters['file_name'] = MY_WORKSPACE_DIR + 'BPI_Challenge_2012.xes.gz' #'BPI_2012_W_complete.xes.gz'\n",
        "    #parameters['model_file'] = 'model_rd_100 Nadam_02-0.90.h5'\n",
        "    parameters['n_size'] = 15 # n-gram size\n",
        "\n",
        "    parameters['log_name'] = 'bpic2012_15_lstm_sufftime'\n",
        "\n",
        "    parameters['task']='prefix_attn111'\n",
        "    \n",
        "    training_model(timeformat, parameters)\n",
        "    \n",
        " \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoRq81SKGl5J",
        "colab_type": "text"
      },
      "source": [
        "#### Model evaluation for suffix and remaining time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzZRoNXNLq2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Mar  8 08:16:15 2019\n",
        "\n",
        "@author: Manuel Camargo\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "\n",
        "from keras.models import load_model\n",
        "import keras.utils as ku\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import jellyfish as jf\n",
        "\n",
        "START_TIMEFORMAT = ''\n",
        "INDEX_AC = None\n",
        "INDEX_RL = None\n",
        "DIM = dict()\n",
        "TBTW = dict()\n",
        "EXP = dict()\n",
        "MY_WORKSPACE_DIR = \"/content/drive/My Drive/BPIC_Data/\"\n",
        "timeformat = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "\n",
        "def predict_suffix_full(timeformat, parameters, is_single_exec=False):\n",
        "    \"\"\"Main function of the suffix prediction module.\n",
        "    Args:\n",
        "        timeformat (str): event-log date-time format.\n",
        "        parameters (dict): parameters used in the training step.\n",
        "        is_single_exec (boolean): generate measurments stand alone or share\n",
        "                    results with other runing experiments (optional)\n",
        "    \"\"\"\n",
        "    global START_TIMEFORMAT\n",
        "    global INDEX_AC\n",
        "    global INDEX_RL\n",
        "    global DIM\n",
        "    global TBTW\n",
        "    global EXP\n",
        "\n",
        "    START_TIMEFORMAT = timeformat\n",
        "\n",
        " \n",
        "\n",
        "    output_route =  parameters['folder']\n",
        "    model_name, _ = os.path.splitext(parameters['model_file'])\n",
        "\n",
        "    # Loading of testing dataframe\n",
        "    \n",
        "    df_test = pd.read_csv(os.path.join(output_route, 'parameters', parameters['log_name']+'test_log.csv'))\n",
        "    df_test['start_timestamp'] = pd.to_datetime(df_test['start_timestamp'])\n",
        "    df_test['end_timestamp'] = pd.to_datetime(df_test['end_timestamp'])\n",
        "    df_test = df_test.drop(columns=['user'])\n",
        "    df_test = df_test.rename(index=str, columns={\"role\": \"user\"})\n",
        "\n",
        "    # Loading of parameters from training\n",
        "    with open(os.path.join(output_route, 'parameters', parameters['log_name']+'model_parameters.json')) as file:\n",
        "        data = json.load(file)\n",
        "        EXP = {k: v for k, v in data['exp_desc'].items()}\n",
        "        print(EXP)\n",
        "        DIM['samples'] = int(data['dim']['samples'])\n",
        "        DIM['time_dim'] = int(data['dim']['time_dim'])\n",
        "        DIM['features'] = int(data['dim']['features'])\n",
        "        TBTW['mean_tbtw'] = float(data['mean_tbtw'])\n",
        "        TBTW['std_tbtw']= float(data['std_tbtw'])\n",
        "        INDEX_AC = {int(k): v for k, v in data['index_ac'].items()}\n",
        "        INDEX_RL = {int(k): v for k, v in data['index_rl'].items()}\n",
        "        file.close()\n",
        "\n",
        "    if EXP['norm_method'] == 'max':\n",
        "        mean_tbtw = np.mean(df_test.tbtw)\n",
        "        std_tbtw = np.std(df_test.tbtw)\n",
        "        norm = lambda x: (x['tbtw']-mean_tbtw)/std_tbtw\n",
        "        df_test['tbtw_norm'] = df_test.apply(norm, axis=1)\n",
        "    elif EXP['norm_method'] == 'lognorm':\n",
        "        logit = lambda x: math.log1p(x['tbtw'])\n",
        "        df_test['tbtw_log'] = df_test.apply(logit, axis=1)\n",
        "        mean_tbtw = np.mean(df_test.tbtw_log)\n",
        "        std_tbtw = np.std(df_test.tbtw_log)\n",
        "        norm = lambda x: (x['tbtw_log']-mean_tbtw)/std_tbtw\n",
        "        df_test['tbtw_norm'] = df_test.apply(norm, axis=1)\n",
        "\n",
        "    ac_index = {v: int(k) for k, v in data['index_ac'].items()}\n",
        "    rl_index = {v: int(k) for k, v in data['index_rl'].items()}\n",
        "\n",
        "    ac_alias = create_alias(len(INDEX_AC))\n",
        "    print('printing ac_alias', ac_alias)\n",
        "    rl_alias = create_alias(len(INDEX_RL))\n",
        "    print('printing rl_alias', rl_alias)\n",
        "\n",
        "#   Next event selection method and numbers of repetitions\n",
        "    var = {'imp': 'Arg Max', 'rep': 1} #,{'imp': 'Random Choice', 'rep': 3}]\n",
        "#   Generation of predictions\n",
        "    model = load_model(os.path.join(output_route, parameters['model_file']))\n",
        "    print('df_test shape', df_test.shape)\n",
        "    \n",
        "    all_prefixes = create_pref_suf_full(df_test, ac_index, rl_index)\n",
        "    print('length of all prefixes', len(all_prefixes))\n",
        "    prefList = chunks(all_prefixes,100)\n",
        "    print('length of all chunks', prefList)\n",
        "\n",
        "    for prefixes in prefList:\n",
        "        \n",
        "        measurements_ac = list()\n",
        "        measurements_rl = list()\n",
        "        measurements_mae = list()\n",
        "\n",
        "        for i in range(0, 1):\n",
        "            #print(i)\n",
        "            #prefixes = create_pref_suf_full(df_test, ac_index, rl_index)\n",
        "            \n",
        "            \n",
        "            prefixes = predict_full(model, prefixes, var['imp'], 100)\n",
        "            prefixes = dl_measure_full(prefixes, 'ac', ac_alias)\n",
        "            prefixes = dl_measure_full(prefixes, 'rl', rl_alias)\n",
        "            prefixes = ae_measure_full(prefixes)\n",
        "            prefixes = pd.DataFrame.from_dict(prefixes)\n",
        "            prefixes = prefixes.groupby('pref_size', as_index=False).agg({'ac_dl': 'mean','rl_dl': 'mean', 'ae': 'mean'})\n",
        "            measure_ac = dict()\n",
        "            measure_rl = dict()\n",
        "            measure_rem = dict()\n",
        "            for size in prefixes.pref_size.unique():\n",
        "                measure_ac[size] = prefixes[prefixes.pref_size==size].ac_dl.iloc[0]\n",
        "                measure_rl[size] = prefixes[prefixes.pref_size==size].rl_dl.iloc[0]\n",
        "                measure_rem[size] = prefixes[prefixes.pref_size==size].ae.iloc[0]\n",
        "            measure_ac['avg'] = prefixes.ac_dl.mean()\n",
        "            print('SUFF-SIM', prefixes.ac_dl.mean())\n",
        "            measure_rl['avg'] = prefixes.rl_dl.mean()\n",
        "            print('ROLE-SIM', prefixes.rl_dl.mean())\n",
        "            measure_rem['avg'] = prefixes.ae.mean()\n",
        "            print('REM-', prefixes.ae.mean())\n",
        "            # Save results\n",
        "            measurements_ac.append({**dict(model=os.path.join(output_route, parameters['log_name']),\n",
        "                                        implementation=var['imp']), **measure_ac,\n",
        "                                **EXP})\n",
        "            measurements_rl.append({**dict(model=os.path.join(output_route, parameters['log_name']),\n",
        "                                        implementation=var['imp']), **measure_rl,\n",
        "                                **EXP})\n",
        "            measurements_mae.append({**dict(model=os.path.join(output_route, parameters['log_name']),\n",
        "                                        implementation=var['imp']), **measure_rem,\n",
        "                                **EXP})\n",
        "        save_results_full(measurements_ac, 'ac', is_single_exec, parameters)\n",
        "        save_results_full(measurements_rl, 'rl', is_single_exec, parameters)\n",
        "        save_results_full(measurements_mae, 'mae', is_single_exec, parameters)\n",
        "    \n",
        "def save_results_full(measurements, feature, is_single_exec, parameters):    \n",
        "    output_route = os.path.join( parameters['folder'])\n",
        "    model_name, _ = os.path.splitext(parameters['model_file'])\n",
        "    if measurements:    \n",
        "        if is_single_exec:\n",
        "                create_csv_file_header(measurements, os.path.join(output_route,\n",
        "                                                                      model_name +'_'+parameters['log_name']+'_full_suff.csv'))\n",
        "        else:\n",
        "            if os.path.exists(os.path.join('output_files', 'full_'+feature+'_suffix_measures.csv')):\n",
        "                create_csv_file(measurements, os.path.join(output_route,\n",
        "                                                               'full_'+parameters['log_name']+'_suffix_measures.csv'), mode='a')\n",
        "            else:\n",
        "                create_csv_file_header(measurements, os.path.join(output_route,\n",
        "                                                               'full_'+parameters['log_name']+'_suffix_measures.csv'))\n",
        "\n",
        "# =============================================================================\n",
        "# Predic traces\n",
        "# =============================================================================\n",
        "\n",
        "def predict_full(model, prefixes, imp, max_trace_size):\n",
        "    \"\"\"Generate business process suffixes using a keras trained model.\n",
        "    Args:\n",
        "        model (keras model): keras trained model.\n",
        "        prefixes (list): list of prefixes.\n",
        "        imp (str): method of next event selection.\n",
        "    \"\"\"\n",
        "\n",
        "    print('length of prefixes:' , len(prefixes))\n",
        "    \n",
        "    # Generation of predictions\n",
        "    count = 0\n",
        "    for prefix in prefixes:\n",
        "        count+=1\n",
        "        #print('printing each prefix', prefix)\n",
        "        # Activities and roles input shape(1,5)\n",
        "        x_ac_ngram = np.append(\n",
        "                np.zeros(DIM['time_dim']),\n",
        "                np.array(prefix['ac_pref']),\n",
        "                axis=0)[-DIM['time_dim']:].reshape((1,DIM['time_dim']))\n",
        "\n",
        "        #print('The variable x_ac_ngram', x_ac_ngram)      \n",
        "        x_rl_ngram = np.append(\n",
        "                np.zeros(DIM['time_dim']),\n",
        "                np.array(prefix['rl_pref']),\n",
        "                axis=0)[-DIM['time_dim']:].reshape((1,DIM['time_dim']))\n",
        "\n",
        "        # times input shape(1,5,1)\n",
        "        x_t_ngram = np.array([np.append(\n",
        "                np.zeros(DIM['time_dim']),\n",
        "                np.array(prefix['t_pref']),\n",
        "                axis=0)[-DIM['time_dim']:].reshape((DIM['time_dim'], 1))])\n",
        "        acum_tbtw = 0\n",
        "        ac_suf, rl_suf = list(), list()\n",
        "        #print('Before for loop', max_trace_size)\n",
        "        for _  in range(1, max_trace_size):\n",
        "            predictions = model.predict([x_ac_ngram, x_rl_ngram, x_t_ngram])\n",
        "            #print(_)\n",
        "            if imp == 'Random Choice':\n",
        "                # Use this to get a random choice following as PDF the predictions\n",
        "                pos = np.random.choice(np.arange(0, len(predictions[0][0])), p=predictions[0][0])\n",
        "                pos1 = np.random.choice(np.arange(0, len(predictions[1][0])), p=predictions[1][0])\n",
        "                #print('pos and pos1', pos, pos1)\n",
        "            elif imp == 'Arg Max':\n",
        "                # Use this to get the max prediction\n",
        "                pos = np.argmax(predictions[0][0])\n",
        "                pos1 = np.argmax(predictions[1][0])\n",
        "            # Activities accuracy evaluation\n",
        "            #print('Printing x_ac_ngram before', x_ac_ngram)\n",
        "            x_ac_ngram = np.append(x_ac_ngram, [[pos]], axis=1)\n",
        "            #print('Printing x_ac_ngram after', x_ac_ngram)\n",
        "            x_ac_ngram = np.delete(x_ac_ngram, 0, 1)\n",
        "            x_rl_ngram = np.append(x_rl_ngram, [[pos1]], axis=1)\n",
        "            x_rl_ngram = np.delete(x_rl_ngram, 0, 1)\n",
        "            x_t_ngram = np.append(x_t_ngram, [predictions[2]], axis=1)\n",
        "            x_t_ngram = np.delete(x_t_ngram, 0, 1)\n",
        "            #print('Printing x_ac_ngram after final', x_ac_ngram)\n",
        "            # Stop if the next prediction is the end of the trace\n",
        "            # otherwise until the defined max_size\n",
        "            #print('Printing ac_suf before', ac_suf)\n",
        "            ac_suf.append(pos)\n",
        "            #print('Printing ac_suf after', ac_suf)\n",
        "            rl_suf.append(pos1)\n",
        "            if EXP['norm_method'] == 'lognorm':\n",
        "                acum_tbtw += math.expm1(predictions[2][0][0] * TBTW['std_tbtw'] + TBTW['mean_tbtw'] )\n",
        "            else:\n",
        "                acum_tbtw += np.rint(predictions[2][0][0] * TBTW['std_tbtw'] + TBTW['mean_tbtw'])\n",
        "            if INDEX_AC[pos] == 'end':\n",
        "                break\n",
        "        prefix['ac_suff_pred'] = ac_suf\n",
        "        prefix['rl_suff_pred'] = rl_suf\n",
        "        prefix['rem_time_pred'] = acum_tbtw\n",
        "        \n",
        "    print_done_task()\n",
        "    return prefixes\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Reformat\n",
        "# =============================================================================\n",
        "def create_pref_suf_full(df_test, ac_index, rl_index):\n",
        "    \"\"\"Extraction of prefixes and expected suffixes from event log.\n",
        "    Args:\n",
        "        df_test (dataframe): testing dataframe in pandas format.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "        pref_size (int): size of the prefixes to extract.\n",
        "    Returns:\n",
        "        list: list of prefixes and expected sufixes.\n",
        "    \"\"\"\n",
        "    prefixes = list()\n",
        "    cases = df_test.caseid.unique()\n",
        "    print('Number of cases' , len(cases))\n",
        "   \n",
        "    for case in cases:\n",
        "       \n",
        "        trace = df_test[df_test.caseid == case].to_dict('records')\n",
        "        \n",
        "        ac_pref = list()\n",
        "        rl_pref = list()\n",
        "        t_pref = list()\n",
        "        for i in range(0, len(trace)-1):\n",
        "            ac_pref.append(trace[i]['ac_index'])\n",
        "            rl_pref.append(trace[i]['rl_index'])\n",
        "            t_pref.append(trace[i]['tbtw_norm'])\n",
        "            prefixes.append(dict(ac_pref=ac_pref.copy(),\n",
        "                                 ac_suff=[x['ac_index'] for x in trace[i + 1:]],\n",
        "                                 rl_pref=rl_pref.copy(),\n",
        "                                 rl_suff=[x['rl_index'] for x in trace[i + 1:]],\n",
        "                                 t_pref=t_pref.copy(),\n",
        "#                                 rem_time=(trace[-1]['end_timestamp'] - trace[i + 1]['start_timestamp']).total_seconds(),\n",
        "                                 rem_time=[x['tbtw'] for x in trace[i + 1:]],\n",
        "                                 pref_size=i + 1))\n",
        "           \n",
        "    for x in prefixes:\n",
        "        x['ac_suff'].append(ac_index['end'])\n",
        "        x['rl_suff'].append(rl_index['end'])\n",
        "        x['rem_time'].append(0)\n",
        "    return prefixes\n",
        "\n",
        "def create_alias(quantity):\n",
        "    \"\"\"Creates char aliases for a categorical attributes.\n",
        "    Args:\n",
        "        quantity (int): number of aliases to create.\n",
        "    Returns:\n",
        "        dict: alias for a categorical attributes.\n",
        "    \"\"\"\n",
        "    characters = [chr(i) for i in range(0, quantity)]\n",
        "    aliases = random.sample(characters, quantity)\n",
        "    alias = dict()\n",
        "    for i in range(0, quantity):\n",
        "        alias[i] = aliases[i]\n",
        "    return alias\n",
        "\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "\n",
        "def dl_measure_full(prefixes, feature, alias):\n",
        "    \"\"\"Demerau-Levinstain distance measurement.\n",
        "    Args:\n",
        "        prefixes (list): list with predicted and expected suffixes.\n",
        "        feature (str): categorical attribute to measure.\n",
        "    Returns:\n",
        "        list: list with measures added.\n",
        "    \"\"\"\n",
        "    for prefix in prefixes:\n",
        "        \n",
        "        suff_log = str([alias[x] for x in prefix[feature + '_suff']])\n",
        "        suff_pred = str([alias[x] for x in prefix[feature + '_suff_pred']])\n",
        "        \n",
        "        length = np.max([len(suff_log), len(suff_pred)])\n",
        "        sim = jf.damerau_levenshtein_distance(suff_log,\n",
        "                                              suff_pred)\n",
        "        sim = (1-(sim/length))\n",
        "        prefix[feature + '_dl'] = sim\n",
        "    return prefixes\n",
        "\n",
        "def ae_measure_full(prefixes):\n",
        "    \"\"\"Absolute Error measurement.\n",
        "    Args:\n",
        "        prefixes (list): list with predicted remaining-times and expected ones.\n",
        "    Returns:\n",
        "        list: list with measures added.\n",
        "    \"\"\"\n",
        "    for prefix in prefixes:\n",
        "        rem_log = np.sum(prefix['rem_time'])\n",
        "#        prefix['ae'] = abs(prefix['rem_time'] - prefix['rem_time_pred'])\n",
        "        prefix['ae'] = abs(rem_log - prefix['rem_time_pred'])\n",
        "    return prefixes\n",
        "\n",
        "\n",
        "MY_WORKSPACE_DIR = \"/content/drive/My Drive/BPIC_Data/\"\n",
        "timeformat = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "parameters = dict()\n",
        "parameters['model_file'] = 'models/model_rd_50 Nadambpic2012_15_lstm_sufftime_28-2.96.h5'\n",
        "\n",
        "parameters['folder'] = \"/content/drive/My Drive/BPIC_Data/output_files/\"\n",
        "#parameters['file_name'] = MY_WORKSPACE_DIR + 'BPI_Challenge_2013.gz'\n",
        "    #parameters['model_file'] = 'model_rd_100 Nadam_02-0.90.h5'\n",
        "parameters['n_size'] = 15 # n-gram size\n",
        "\n",
        "parameters['log_name'] = 'bpic2012_15_lstm_sufftime'\n",
        "\n",
        "predict_suffix_full(timeformat, parameters,is_single_exec=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGZ_AZBNH8hp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sqUKEafLhhW",
        "colab_type": "text"
      },
      "source": [
        "#### Model evaluation for next activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_9-b5_EzqN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "##### Reused existing code for post processing with modifications to get attention weights\n",
        "\"\"\"\n",
        "Created on Fri Mar  8 08:16:15 2019\n",
        "\n",
        "@author: Manuel Camargo\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "\n",
        "from keras.models import load_model\n",
        "#from keract import get_activations\n",
        "\n",
        "import keras.utils as ku\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import jellyfish as jf\n",
        "\n",
        "\n",
        "MY_WORKSPACE_DIR = \"/content/drive/My Drive/BPIC_Data/\"\n",
        "START_TIMEFORMAT = ''\n",
        "INDEX_AC = None\n",
        "INDEX_RL = None\n",
        "DIM = dict()\n",
        "TBTW = dict()\n",
        "EXP = dict()\n",
        "timeformat = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "\n",
        "\n",
        "def predict_next(timeformat, parameters, is_single_exec=True):\n",
        "    \"\"\"Main function of the suffix prediction module.\n",
        "    Args:\n",
        "        timeformat (str): event-log date-time format.\n",
        "        parameters (dict): parameters used in the training step.\n",
        "        is_single_exec (boolean): generate measurments stand alone or share\n",
        "                    results with other runing experiments (optional)\n",
        "    \"\"\"\n",
        "    global START_TIMEFORMAT\n",
        "    global INDEX_AC\n",
        "    global INDEX_RL\n",
        "    global DIM\n",
        "    global TBTW\n",
        "    global EXP\n",
        "\n",
        "    START_TIMEFORMAT = timeformat\n",
        "\n",
        "    \n",
        "    output_route =  parameters['folder']\n",
        "    model_name, _ = os.path.splitext(parameters['model_file'])\n",
        "\n",
        "    # Loading of parameters from training\n",
        "    with open(os.path.join(output_route, 'parameters', parameters['log_name']+'model_parameters.json')) as file:\n",
        "        data = json.load(file)\n",
        "        EXP = {k: v for k, v in data['exp_desc'].items()}\n",
        "        print(EXP)\n",
        "        DIM['samples'] = int(data['dim']['samples'])\n",
        "        DIM['time_dim'] = int(data['dim']['time_dim'])\n",
        "        DIM['features'] = int(data['dim']['features'])\n",
        "        \n",
        "        TBTW['mean_tbtw'] = float(data['mean_tbtw'])\n",
        "        INDEX_AC = {int(k): v for k, v in data['index_ac'].items()}\n",
        "        INDEX_RL = {int(k): v for k, v in data['index_rl'].items()}\n",
        "        file.close()\n",
        "        \n",
        "\n",
        "    ###############Load the training data for LIME/SHAP\n",
        "    #train_vec = pickle.load(open( os.path.join(output_route,\n",
        "    #                                        'parameters',\n",
        "    #                                      parameters['log_name']+ 'train_vec.pkl'), 'rb'))\n",
        "\n",
        "    #ac_input = train_vec['prefixes']['x_ac_inp']\n",
        "\n",
        "    #rl_input = train_vec['prefixes']['x_rl_inp']\n",
        "    #t_input = train_vec['prefixes']['xt_inp']\n",
        "    #y_train = np.argmax(train_vec['next_evt']['y_ac_inp'], axis=1)\n",
        "    #x_train=ac_input\n",
        "\n",
        "    # Loading of testing dataframe\n",
        "    df_test = pd.read_csv(os.path.join(output_route, 'parameters', parameters['log_name']+'test_log.csv'))\n",
        "    df_test['start_timestamp'] = pd.to_datetime(df_test['start_timestamp'])\n",
        "    df_test['end_timestamp'] = pd.to_datetime(df_test['end_timestamp'])\n",
        "    df_test = df_test.drop(columns=['user'])\n",
        "    df_test = df_test.rename(index=str, columns={\"role\": \"user\"})\n",
        "\n",
        "    \n",
        "\n",
        "    if EXP['norm_method'] == 'max':\n",
        "        mean_tbtw = np.mean(df_test.tbtw)\n",
        "        std_tbtw=np.std(df_test.tbtw)\n",
        "        norm = lambda x: (x['tbtw']-mean_tbtw)/std_tbtw\n",
        "        df_test['tbtw_norm'] = df_test.apply(norm, axis=1)\n",
        "    elif EXP['norm_method'] == 'lognorm':\n",
        "        logit = lambda x: math.log1p(x['tbtw'])\n",
        "        df_test['tbtw_log'] = df_test.apply(logit, axis=1)\n",
        "        mean_tbtw = np.mean(df_test.tbtw_log)\n",
        "        std_tbtw=np.std(df_test.tbtw_log)\n",
        "        norm = lambda x: (x['tbtw_log']-mean_tbtw)/std_tbtw\n",
        "        df_test['tbtw_norm'] = df_test.apply(norm, axis=1)\n",
        "\n",
        "\n",
        "    print(INDEX_AC)\n",
        "   \n",
        "\n",
        "#   Next event selection method and numbers of repetitions\n",
        "    variants = [{'imp': 'Arg Max', 'rep': 1}]#,\n",
        "               # {'imp': 'Random Choice', 'rep': 1}]\n",
        "#   Generation of predictions\n",
        "    has_time=False\n",
        "    model = load_model(os.path.join(output_route, parameters['model_file']))\n",
        "    layer_names = [layer.name for layer in model.layers]\n",
        "    print(layer_names)\n",
        "    rl_emb_weights=None\n",
        "    ac_emb_weights = model.get_layer(name='ac_embedding').get_weights()[0]\n",
        "    if 'rl_embedding' in layer_names:\n",
        "      rl_emb_weights = model.get_layer(name='rl_embedding').get_weights()[0]\n",
        "    if 't_input' in layer_names:\n",
        "      has_time=True\n",
        "    #print(rl_emb_weights)\n",
        "    ac_output_weights, ac_bias = model.get_layer(name='act_output').get_weights()\n",
        "    print(ac_output_weights)\n",
        "    prefix_only=False\n",
        "    if(parameters['attention']=='prefix'):\n",
        "        model_with_attention = Model(model.inputs, model.outputs +\\\n",
        "                                              [model.get_layer(name='alpha_softmax').output])\n",
        "        prefix_only=True\n",
        "    else:\n",
        "        model_with_attention = Model(model.inputs, model.outputs +\\\n",
        "                                              [model.get_layer(name='alpha_softmax').output,\\\n",
        "                                               model.get_layer(name='beta_dense_0').output])\n",
        "        \n",
        "    \n",
        "\n",
        "    for var in variants:\n",
        "        measurements = list()\n",
        "        for i in range(0, 1):\n",
        "            print(var['imp'])\n",
        "            prefixes= create_pref_suf(df_test)\n",
        "            #if temporal attention True, else False\n",
        "            \n",
        "            prefixes, temporal_vectors, variable_vectors  = predict_next_in(model_with_attention, ac_emb_weights, rl_emb_weights, ac_output_weights, has_time, prefixes,var['imp'], prefix_only)\n",
        "            \n",
        "            accuracy = (np.sum([x['ac_true'] for x in prefixes])/len(prefixes))\n",
        "            print(accuracy)\n",
        "            y_pred = [x['ac_pred'] for x in prefixes]\n",
        "            y_true = [x['ac_next'] for x in prefixes]\n",
        "\n",
        "            from sklearn.metrics import classification_report\n",
        "            print(classification_report(y_true, y_pred))\n",
        "\n",
        "            file_name='results/'+ parameters['log_name']+'next_event_measures.csv'\n",
        "            # Save results\n",
        "            measurements.append({**dict(model=os.path.join(output_route, file_name),\n",
        "                                        implementation=var['imp']), **{'accuracy': accuracy},\n",
        "                                **EXP})\n",
        "            if measurements:    \n",
        "                if os.path.exists(os.path.join(output_route, file_name)):\n",
        "                    create_csv_file(measurements, os.path.join(output_route, file_name), mode='a')\n",
        "                else:\n",
        "                    create_csv_file_header(measurements, os.path.join(output_route,file_name))\n",
        "  \n",
        "    #print(attention_vector_final)\n",
        "    file_name = parameters['log_name'] + str(DIM['time_dim'])\n",
        "    path = output_route + '/results/'\n",
        "    temp_final = np.mean(np.array(temporal_vectors), axis=0)\n",
        "    pd.DataFrame(temp_final, columns=['alpha attention weight']).plot(kind='bar',\n",
        "                                                                        title='Attention of '\n",
        "                                                                               ' index')\n",
        "    \n",
        "    plot_history( plt, file_name + 'prefix_attn', path )\n",
        "    plt.show()\n",
        "    if(len(variable_vectors)>0):\n",
        "      var_final = np.mean(np.array(variable_vectors), axis=0)\n",
        "\n",
        "\n",
        "      ac_labels = [INDEX_AC[key] for key in sorted(INDEX_AC.keys())]\n",
        "      rl_labels = [INDEX_RL[key] for key in sorted(INDEX_RL.keys())]\n",
        "      print(ac_labels)\n",
        "      \n",
        "      num_dim = var_final.shape[0]\n",
        "      print(num_dim)\n",
        "      \n",
        "      if rl_emb_weights is not None:\n",
        "        ac_labels.extend(rl_labels)\n",
        "      if(num_dim==len(ac_labels)+1):\n",
        "        ac_labels.append('time')\n",
        "        \n",
        "      df_var=pd.DataFrame({'attributes':var_final, 'attribute_values':ac_labels})\n",
        "    #print(df_var)\n",
        "      df_var.plot.bar(y='attributes', x='attribute_values',\n",
        "                                title='Attention of the event attributes.', figsize=(10,5))\n",
        "                                                                               \n",
        "      plot_history( plt, file_name + 'variable_attn', path )\n",
        "      \n",
        "\n",
        "      plt.show()\n",
        "\n",
        "      #var_local = np.array(variable_vectors[2919])\n",
        "      #df_var_local=pd.DataFrame({'attributes':var_local, 'attribute_values':ac_labels})\n",
        "      #df_var_local.plot.bar(y='attributes', x='attribute_values',\n",
        "      #                          title='Attention of the event attributes.')\n",
        "                                                                               \n",
        "      #plt.show()\n",
        "      #plotting local explanation\n",
        "      #local_var = np.array(temporal_vectors[2919])\n",
        "      #pd.DataFrame(local_var, columns=['alpha attention weight']).plot(kind='bar',\n",
        "      #                                                                  title='Attention of '\n",
        "      #                                                                         ' index')                                                             \n",
        "      #plt.show()\n",
        "      return model\n",
        "# =============================================================================\n",
        "# Predic traces\n",
        "# =============================================================================\n",
        "\n",
        "def predict_next_in(model_attn, ac_emb_weights, rl_emb_weights, ac_output_weights, has_time, prefixes, imp, prefix_only=False):\n",
        "    \"\"\"Generate business process suffixes using a keras trained model.\n",
        "    Args:\n",
        "        model (keras model): keras trained model.\n",
        "        prefixes (list): list of prefixes.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "        imp (str): method of next event selection.\n",
        "    \"\"\"\n",
        "    # Generation of predictions\n",
        "    temporal_vectors = []\n",
        "    variable_vectors=[]\n",
        "    x_test=[]\n",
        "    y_test=[]\n",
        "    t_dim = DIM['time_dim']\n",
        "    f_dim = DIM['features']\n",
        "    x_test_pos = np.empty((0,t_dim,f_dim))\n",
        "    x_test_neg = np.empty((0,t_dim,f_dim))\n",
        "    \n",
        "\n",
        "    for prefix in prefixes:\n",
        "\n",
        "        # Activities and roles input shape(1,5)\n",
        "        x_ac_ngram = np.append(\n",
        "                np.zeros(DIM['time_dim']),\n",
        "                np.array(prefix['ac_pref']),\n",
        "                axis=0)[-DIM['time_dim']:].reshape((1,DIM['time_dim']))\n",
        "                \n",
        "        x_rl_ngram = np.append(\n",
        "                np.zeros(DIM['time_dim']),\n",
        "                np.array(prefix['rl_pref']),\n",
        "                axis=0)[-DIM['time_dim']:].reshape((1,DIM['time_dim']))\n",
        "\n",
        "        # times input shape(1,5,1)\n",
        "        x_t_ngram = np.array([np.append(\n",
        "                np.zeros(DIM['time_dim']),\n",
        "                np.array(prefix['t_pref']),\n",
        "                axis=0)[-DIM['time_dim']:].reshape((DIM['time_dim'], 1))])\n",
        "\n",
        "        betas=None      \n",
        "        #proba = model.predict(x_ac_ngram)\n",
        "          \n",
        "       \n",
        "        if prefix_only:\n",
        "          proba, alphas = model_attn.predict([x_ac_ngram, x_rl_ngram, x_t_ngram])\n",
        "        else:\n",
        "          if rl_emb_weights is not None and has_time==True:\n",
        "            proba, alphas, betas = model_attn.predict([x_ac_ngram, x_rl_ngram, x_t_ngram])\n",
        "          elif rl_emb_weights is not None and has_time==False:\n",
        "            proba, alphas, betas = model_attn.predict([x_ac_ngram, x_rl_ngram])\n",
        "          else:\n",
        "            proba, alphas, betas = model_attn.predict([x_ac_ngram])\n",
        "        #print(proba, alphas, betas)\n",
        "        proba = np.squeeze(proba)\n",
        "        alphas = np.squeeze(alphas)\n",
        "        temporal_att_vec = alphas\n",
        "        assert (np.sum(temporal_att_vec) - 1.0) < 1e-5\n",
        "        #print(temporal_att_vec)\n",
        "        temporal_vectors.append(temporal_att_vec)\n",
        "\n",
        "        if betas is not None:\n",
        "          #get the beta value\n",
        "          betas = np.squeeze(betas)\n",
        "          idx = np.argmax(alphas)\n",
        "          #print(idx)\n",
        "          beta_val = betas[idx]\n",
        "          # get the activity and role for that idx\n",
        "          act_ip = int(x_ac_ngram[0][idx])\n",
        "          ac_emb = ac_emb_weights[act_ip]\n",
        "          dim=ac_emb.shape[0]\n",
        "          emb=ac_emb\n",
        "\n",
        "          if rl_emb_weights is not None:\n",
        "            rol_ip = int(x_rl_ngram[0][idx])\n",
        "            r_emb = rl_emb_weights[rol_ip]\n",
        "            dim = dim+ r_emb.shape[0]\n",
        "            emb = np.concatenate((ac_emb,r_emb), axis=None)\n",
        "          \n",
        "          if(betas.shape[1]==dim+1):\n",
        "            time_v = np.squeeze(x_t_ngram)[idx]  # time and role as masked together\n",
        "            emb = np.concatenate((ac_emb,r_emb,time_v), axis=None)\n",
        "          \n",
        "          #print('beta_val',beta_val.shape)\n",
        "          beta_scaled = np.multiply(beta_val,emb)\n",
        "          variable_attn = alphas[idx] * beta_scaled\n",
        "          #sum_grad = np.sum(ac_output_weights, axis=1)\n",
        "          #variable_attn=np.multiply(sum_grad.flatten(), variable_attn)\n",
        "         \n",
        "          variable_vectors.append(variable_attn)\n",
        "        \n",
        "        \n",
        "        \n",
        "        if imp == 'Random Choice':\n",
        "            # Use this to get a random choice following as PDF the predictions\n",
        "            pos = np.argmax(proba)\n",
        "            \n",
        "        elif imp == 'Arg Max':\n",
        "            # Use this to get the max prediction\n",
        "            pos = np.argmax(proba)\n",
        "\n",
        "        prefix['ac_pred']  = pos\n",
        "        # Activities accuracy evaluation\n",
        "        if pos == prefix['ac_next']:\n",
        "            prefix['ac_true'] = 1\n",
        "            if(idx<4):\n",
        "              print('value is ', len(variable_vectors))\n",
        "        else:\n",
        "            prefix['ac_true'] = 0\n",
        "           # x_test_neg = np.append(x_test_neg,x_ac_ngram, axis=0)\n",
        "        ####get the temporal attention\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        #attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
        "        # plot part.\n",
        "    \n",
        "    #print_done_task()\n",
        "    return prefixes, temporal_vectors, variable_vectors\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Reformat\n",
        "# =============================================================================\n",
        "def create_pref_suf(df_test):\n",
        "    \"\"\"Extraction of prefixes and expected suffixes from event log.\n",
        "    Args:\n",
        "        df_test (dataframe): testing dataframe in pandas format.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "        pref_size (int): size of the prefixes to extract.\n",
        "    Returns:\n",
        "        list: list of prefixes and expected sufixes.\n",
        "    \"\"\"\n",
        "    prefixes = list()\n",
        "    cases = df_test.caseid.unique()\n",
        "    for case in cases:\n",
        "        trace = df_test[df_test.caseid == case]\n",
        "        ac_pref = list()\n",
        "        rl_pref = list()\n",
        "        t_pref = list()\n",
        "        for i in range(0, len(trace)-1):\n",
        "            ac_pref.append(trace.iloc[i]['ac_index'])\n",
        "            rl_pref.append(trace.iloc[i]['rl_index'])\n",
        "            t_pref.append(trace.iloc[i]['tbtw_norm'])\n",
        "            prefixes.append(dict(ac_pref=ac_pref.copy(),\n",
        "                                 ac_next=trace.iloc[i + 1]['ac_index'],\n",
        "                                 rl_pref=rl_pref.copy(),\n",
        "                                 rl_next=trace.iloc[i + 1]['rl_index'],\n",
        "                                 t_pref=t_pref.copy()))\n",
        "    return prefixes\n",
        "\n",
        "\n",
        "def ae_measure(prefixes):\n",
        "    \"\"\"Absolute Error measurement.\n",
        "    Args:\n",
        "        prefixes (list): list with predicted remaining-times and expected ones.\n",
        "    Returns:\n",
        "        list: list with measures added.\n",
        "    \"\"\"\n",
        "    for prefix in prefixes:\n",
        "        prefix['ae'] = abs(prefix['rem_time'] - prefix['rem_time_pred'])\n",
        "    return prefixes\n",
        "\n",
        "\n",
        "parameters = dict()\n",
        "#   Parameters setting manual fixed or catched by console for batch operations\n",
        "   \n",
        "parameters['folder'] = \"/content/drive/My Drive/BPIC_Data/output_files/\"\n",
        "#parameters['log_name']='bpic2013_15len_50lstm_suftime'\n",
        "parameters['attention']= 'prefix12'\n",
        "parameters['model_file'] = 'models/model_rd_5 Nadamhelpdesk_5lstmARO_full_26-0.42.h5' #'models/model_rd_100 Nadam helpdesk_full_17-0.42.h5'\n",
        "\n",
        "parameters['file_name'] = MY_WORKSPACE_DIR + 'Helpdesk.xes.gz' #'BPI_2012_W_complete.xes.gz' #'BPI_Challenge_2012.xes.gz' #'BPI_Challenge_2013.xes.gz' #'T_BPIC15_5.xes.gz' ##  #BPI_2012_W_complete.xes.gz  #Helpdesk.xes.gz  #\n",
        "    #parameters['model_file'] = 'model_rd_100 Nadam_02-0.90.h5'\n",
        "parameters['n_size'] = 5 # n-gram size\n",
        "\n",
        "parameters['log_name'] = 'helpdesk_5lstmARO_full'\n",
        "\n",
        "parameters['task']='full_attn'\n",
        "\n",
        "\n",
        "model = predict_next(timeformat, parameters,is_single_exec=False)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}